{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Deep Generative Stochastic Networks Trainable by Backprop\n",
      "- [paper link](http://arxiv.org/abs/1306.1091)\n",
      "- [code link](https://github.com/yaoli/DSN)\n",
      "- [discussion](http://fastml.com/running-things-on-a-gpu/)\n",
      "- [paper](files/papers/DeepGenerativeStochasticNetworksTrainablebyBackprop.pdf)\n",
      "- about: building generative model for data (e.g., imputating missing values and etc)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Phraug - online preprocessor for large text file\n",
      "- [code](https://github.com/zygmuntz/phraug)\n",
      "- [discussion](http://fastml.com/introducing-phraug/)\n",
      "- [discussion](http://fastml.com/processing-large-files-line-by-line/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Vowpal Wabbit - Machine Learning Machine Gun\n",
      "- [code](https://github.com/JohnLangford/vowpal_wabbit/wiki)\n",
      "- [mainpage](http://hunch.net/~vw/)\n",
      "- [vw in practice](http://fastml.com/blog/categories/vw/)\n",
      "- [vw for neural network](http://fastml.com/go-non-linear-with-vowpal-wabbit/)\n",
      "- [vw-varinfo for feature selection](https://github.com/JohnLangford/vowpal_wabbit/wiki/using-vw-varinfo)\n",
      "    - [l1 lasso](http://fastml.com/large-scale-l1-feature-selection-with-vowpal-wabbit/)\n",
      "    - [protein prediction](http://stats.stackexchange.com/questions/28877/finding-the-best-features-in-interaction-models)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sofia-ML - Another Machine Learning tools aimed at fast & large-scale learning\n",
      "- [ml code](https://code.google.com/p/sofia-ml/)\n",
      "- [kmeans code](https://code.google.com/p/sofia-ml/wiki/SofiaKMeans)\n",
      "- [paper about using clustering for feature engineering](http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf)\n",
      "- Feature Mapping by Clustering: \n",
      "    - Mapping in this context means representing each data point by its distances to all the centers. Or better yet, by radial basis functions of these distances. It means that when a distance is zero, a feature value is one.\n",
      "    - It is important to use many clusters (think thousands), so the resulting representation becomes that many dimensional. We compensate for it by using a fast linear learner. It seems that a more sophisticated model, like a random forest, can also be trained on mapped data, but it will be much slower.\n",
      "    - Sparse filtering is better than K-means mapping with high-dimensional datasets, because the latter is subject to a curse of dimensionality and works with hundreds of features, tops."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Perf - Performance metrics calcuation tool for binary classification\n",
      "- [mainpage](http://osmot.cs.cornell.edu/kddcup/software.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Matrix Factorization for recommendation system\n",
      "- [tutorial](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/)\n",
      "- [article-recommendation system in general](http://research.yahoo.com/files/ieeecomputer.pdf)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## a theano implementation\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import numpy as np\n",
      "from scipy import optimize\n",
      "\n",
      "class MatrixFactorization(object):\n",
      "    def __init__(self, n, m, k, R, index, beta = 0.1):\n",
      "        self.theta = theano.shared(\n",
      "                    value = np.random.randn(n*k + m*k),\n",
      "                    borrow = True,\n",
      "                    name = 'theta')\n",
      "        self.P = self.theta[:n*k].reshape((n, k))\n",
      "        self.Q = self.theta[n*k:].reshape((m, k))\n",
      "        self.R_tilde = T.dot(self.P, self.Q.T)\n",
      "        ## index -- boolean matrix\n",
      "        self.mse = T.mean((self.R_tilde[index.nonzero()] - R[index]) ** 2)\n",
      "        self.l2 = T.mean(self.P**2) + T.mean(self.Q**2)\n",
      "        self.cost = self.mse + beta * self.l2\n",
      "        self.gparams = T.grad(self.cost, self.theta)\n",
      "        \n",
      "def share_data(data):\n",
      "    shared_data = theano.shared(value = data, borrow = True)\n",
      "    return share_data\n",
      "\n",
      "## test case\n",
      "R = np.asarray([\n",
      "    [5,3,0,1],\n",
      "    [4,0,0,1],\n",
      "    [1,1,0,5],\n",
      "    [1,0,0,4],\n",
      "    [0,1,5,4],\n",
      "])\n",
      "n, m = R.shape\n",
      "k = 2\n",
      "index = R != 0\n",
      "mf = MatrixFactorization(n, m, k, R, index)\n",
      "\n",
      "def f(theta_value):\n",
      "    mf.theta.set_value(theta_value, borrow = True)\n",
      "    return mf.cost.eval()\n",
      "def fprime(theta_value):\n",
      "    mf.theta.set_value(theta_value, borrow = True)\n",
      "    return mf.gparams.eval()\n",
      "theta0 = np.random.randn(n*k + m*k)\n",
      "best_theta = optimize.fmin_l_bfgs_b(func = f, x0 = theta0, fprime = fprime, iprint=1)[0]\n",
      "print best_theta\n",
      "\n",
      "mf.theta.set_value(best_theta, borrow = True)\n",
      "print mf.P.eval()\n",
      "print mf.Q.eval()\n",
      "print mf.R_tilde.eval()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.05211058 -2.24068509 -0.0544423  -1.7784696  -2.25868965 -0.56335438\n",
        " -1.78642043 -0.53384437 -1.87349095 -0.62452512  0.09904446 -2.19400344\n",
        " -0.10852147 -1.2999684  -2.30587502 -0.76852742 -2.02677146 -0.49542942]\n",
        "[[ 0.05211058 -2.24068509]\n",
        " [-0.0544423  -1.7784696 ]\n",
        " [-2.25868965 -0.56335438]\n",
        " [-1.78642043 -0.53384437]\n",
        " [-1.87349095 -0.62452512]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.09904446 -2.19400344]\n",
        " [-0.10852147 -1.2999684 ]\n",
        " [-2.30587502 -0.76852742]\n",
        " [-2.02677146 -0.49542942]]\n",
        "[[ 4.92123205  2.9071647   1.60186745  1.00448509]\n",
        " [ 3.89657619  2.31786244  1.49233978  0.99144826]\n",
        " [ 1.01229074  0.9774592   5.64120933  4.85695006]\n",
        " [ 0.99432133  0.88784578  4.52953627  3.88514814]\n",
        " [ 1.18465135  1.01517691  4.80000066  4.1065461 ]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Practical Machine Learning from KDD\n",
      "- [kdd-2011-best-paper](http://blog.david-andrzejewski.com/machine-learning/practical-machine-learning-tricks-from-the-kdd-2011-best-industry-paper/)\n",
      "- [sparse machine learning](http://www.di.ens.fr/~fbach/cvpr_tutorial_2010_sparse_methods_ML.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# mRMR - minimum Redundancy Maximum Relevance FEATURE SELECTION \n",
      "- [mainpage](http://penglab.janelia.org/proj/mRMR/)\n",
      "- [discussion](http://fastml.com/feature-selection-in-practice/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sparse Linear Selection + Linear Model (Or Not-so-deep a.k.a one-layer Model)\n",
      "- [example on blackbox learning](http://fastml.com/more-on-sparse-filtering-and-the-black-box-competition/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Spearmint - automatic hyper-parameter tunning\n",
      "- [code](https://github.com/JasperSnoek/spearmint)\n",
      "- [mainsite](http://www.cs.toronto.edu/~jasper/software.html)\n",
      "- [discussion](http://fastml.com/tuning-hyperparams-automatically-with-spearmint/)\n",
      "- [discussion](http://fastml.com/tuning-hyperparams-automatically-with-spearmint/)\n",
      "- [discussion](http://fastml.com/madelon-spearmints-revenge/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sparse Filtering - powerful feature extraction Tech\n",
      "- [code & Mainpage](http://cs.stanford.edu/~jngiam/)\n",
      "- [paper](http://cs.stanford.edu/~jngiam/papers/NgiamKohChenBhaskarNg2011.pdf)\n",
      "- [python code](https://bitbucket.org/mhorbal/learningtools)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Deep Learning (dropout/maxout method) - big deep network on small data \n",
      "- [dropout paper](http://arxiv.org/abs/1207.0580)\n",
      "- [maxout paper](http://arxiv.org/abs/1302.4389)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GraphChi - disk-based large-scale graph computation\n",
      "- [code](https://code.google.com/p/graphchi/)\n",
      "- [blog](http://bickson.blogspot.sg/2012/12/collaborative-filtering-with-graphchi.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# dimension reduction for binary data (e.g. text)\n",
      "- [review](http://fastml.com/dimensionality-reduction-for-sparse-binary-data-an-overview/)\n",
      "- [review](http://fastml.com/dimensionality-reduction-for-sparse-binary-data/)\n",
      "- [gensim](http://radimrehurek.com/gensim/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sklearn Roadmap\n",
      "http://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Blogs and Deep learning code in python\n",
      "\n",
      "http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/\n",
      "\n",
      "http://www.iro.umontreal.ca/~memisevr/\n",
      "\n",
      "https://www.ipam.ucla.edu/schedule.aspx?pc=gss2012\n",
      "\n",
      "http://www.cs.toronto.edu/~rfm/multiview-feature-learning-cvpr/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}