{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clustering Methods\n",
      "\n",
      "**Questions to ask**\n",
      "\n",
      "1. what precondition is for each clustering method (e.g. normalization, non-binary, distance matrix and etc)\n",
      "2. what assumption is for each clustering method (e.g. large N, midum D, and medium cluster #)\n",
      "3. how do we tell if a clustering result is useful (e.g. unsupervised metric, or supervised metric for feature learning\n",
      "\n",
      "**Common Data types**\n",
      "\n",
      "1. binary sparse data - from text or discretization\n",
      "2. image data\n",
      "3. time series\n",
      "4. continouse value data\n",
      "\n",
      "**Common clustering metric**\n",
      "\n",
      "1. adjusted_mutual_information_score with true TARGET - NMI is mostly common in the literature, it ranges to [0, 1], but it is NOT adjusted to chance. AMI was recently proposed, its specially adjusted to pure chance, but it could have negative values. MI is NOT really widly used - if the metric is NOT adjusted for chance, its value will tend to increase as the number of clusters increase, regardless of the actual amout of mutual information between the labels and cluster assignments\n",
      "\n",
      "2. confusion matrix between target and cluster: ideally each true class should appear in several clusters alone, and exclude other classes in the same cluster. In detail, we care about:\n",
      "    - homogeneity: each cluster contains only members of a single class.\n",
      "    - completeness: all members of a given class are assigned to the same cluster.\n",
      "    - when using a large number of clusters, we are more interested in homogeneity\n",
      "\n",
      "Use `metrics.homogeneity_score(labels_true, labels_pred) ` and `metrics.completeness_score(labels_true, labels_pred) ` to calculate. Their harmonic mean called V-measure is computed by v_measure_score: `metrics.v_measure_score(labels_true, labels_pred)`\n",
      "\n",
      "**The V-measure is actually equivalent to the mutual information (NMI) discussed above normalized by the sum of the label entropies** Again it is NOT adjusted to the chance, so it suffers from increasing number of clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import numpy as np\n",
      "from sklearn import cluster\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "from sklearn import decomposition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## MNIST DATA\n",
      "(mnist_X, mnist_y), _, _ = cPickle.load(open('data/mnist.pkl'))\n",
      "print mnist_X.shape\n",
      "print np.unique(mnist_y)\n",
      "print mnist_X.mean(axis = 0).std() ## how different is every attribute"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50000, 784)\n",
        "[0 1 2 3 4 5 6 7 8 9]\n",
        "0.166019244572"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## 20 news group data\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import re\n",
      "\n",
      "newsgroup = fetch_20newsgroups()\n",
      "ng_X, ng_y = newsgroup.data, newsgroup.target\n",
      "def number_aware_tokenizer(doc):\n",
      "    number_pattern = re.compile(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
      "    words = [\"#NUM\" if w[0] in '0123456789_' else w for w in number_pattern.findall(doc)]\n",
      "    return words\n",
      "\n",
      "## min_df = 5, max_df = 0.5 is very common setting for a lot feature extraction tasks\n",
      "## analzyer: preprocessor + tokenizer\n",
      "## preprocessor -> preprocess before tokenize\n",
      "## USE norm = None to avoid l2 normalization on a sample basis\n",
      "vectorizer = TfidfVectorizer(tokenizer = number_aware_tokenizer, max_df = 0.5, stop_words = 'english', norm=None)\n",
      "ng_X = vectorizer.fit_transform(ng_X)\n",
      "print ng_X.shape\n",
      "print np.unique(ng_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(11314, 105155)\n",
        "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## black box data\n",
      "black_X, black_y = cPickle.load(open('data/blackbox.pkl'))\n",
      "print black_X.shape\n",
      "print np.unique(black_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000, 1875)\n",
        "[1 2 3 4 5 6 7 8 9]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## KMeans / Minibatch KMeans\n",
      "\n",
      "- scale up to large number of examples\n",
      "- UNIVERSALLY applicable if Euclidean distance makes sense \n",
      "- Usually gives good results even when Euclidean distance does NOT make sense, e.g. for text data\n",
      "- You may want to try PCA first when the dimensionality is very high - but it will ruin the sparseness\n",
      "- for feature learning, it is good idea to use a large number of clusters then use soft-thresholding or tri-cluster to do then ENCODING\n",
      "- make sure to NORMALIZE features at least before using kmeans\n",
      "- if Minibatch kmeans is used, ideally the larger the batch size, the faster it converges, but the higher the computation cost"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TEST kmeans (MiniBatch KMEANS) on MNIST\n",
      "## the feature-wise normalization in NOT really necessary as the difference between\n",
      "## different features are NOT significant\n",
      "\n",
      "X = mnist_X\n",
      "y = mnist_y\n",
      "n_clusters = len(np.unique(y))\n",
      "\n",
      "## feature wise normalization\n",
      "normalized_X = preprocessing.scale(X, axis = 0, )\n",
      "sample_normal_X = preprocessing.normalize(X, norm='l2', axis = 1)\n",
      "pca_X = decomposition.PCA(n_components=50, whiten = False).fit_transform(X)\n",
      "pca_whiten_X = decomposition.PCA(n_components=50, whiten = True).fit_transform(X)\n",
      "\n",
      "## kmeans++ / raw data\n",
      "%time raw_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=5000, random_state=0).fit(X).labels_)\n",
      "print('kmeans++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, raw_clusters), \n",
      "      metrics.homogeneity_score(y, raw_clusters),\n",
      "      metrics.completeness_score(y, raw_clusters))\n",
      "## kmeans++ / normalized data\n",
      "%time normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=5000, random_state=0).fit(normalized_X).labels_)\n",
      "print('kmeans++/normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, normal_clusters), \n",
      "      metrics.homogeneity_score(y, normal_clusters),\n",
      "      metrics.completeness_score(y, normal_clusters))\n",
      "## kmeans++ / sample normalized data\n",
      "%time sample_normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=5000, random_state=0).fit(sample_normal_X).labels_)\n",
      "print('kmeans++/sample normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, sample_normal_clusters), \n",
      "      metrics.homogeneity_score(y, sample_normal_clusters),\n",
      "      metrics.completeness_score(y, sample_normal_clusters))\n",
      "## pca initialization / raw data\n",
      "pca_components = decomposition.PCA(n_components=n_clusters).fit(X).components_\n",
      "%time pcainit_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, init=pca_components, batch_size=5000, random_state=0).fit(X).labels_)\n",
      "print('pca++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pcainit_clusters), \n",
      "      metrics.homogeneity_score(y, pcainit_clusters),\n",
      "      metrics.completeness_score(y, pcainit_clusters))\n",
      "## kmeans++ / pca data\n",
      "%time pca_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=5000, random_state=0).fit(pca_X).labels_)\n",
      "print('kmeans++/pca data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_clusters), \n",
      "      metrics.homogeneity_score(y, pca_clusters),\n",
      "      metrics.completeness_score(y, pca_clusters))\n",
      "## kmeans++ / pca whiten data\n",
      "%time pca_whiten_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=5000, random_state=0).fit(pca_whiten_X).labels_)\n",
      "print('kmeans++/pca whiten data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_whiten_clusters), \n",
      "      metrics.homogeneity_score(y, pca_whiten_clusters),\n",
      "      metrics.completeness_score(y, pca_whiten_clusters))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 17.4 s, sys: 4.1 s, total: 21.5 s\n",
        "Wall time: 21 s\n",
        "('kmeans++/raw data:', 0.42344525971552349, 0.42364690396131016, 0.43240891812453053)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 13.9 s, sys: 4.14 s, total: 18 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 17.6 s\n",
        "('kmeans++/normalized data:', 0.37784028738459213, 0.37806203638077251, 0.41531906855247469)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 23.8 s, sys: 4.02 s, total: 27.8 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 27.4 s\n",
        "('kmeans++/sample normalized data:', 0.47885767809483487, 0.47903994041164172, 0.49007730118937409)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 14.1 s, sys: 1.38 s, total: 15.5 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 15.4 s\n",
        "('pca++/raw data:', 0.37118314255868484, 0.37140305989810951, 0.37687710200919977)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 1.26 s, sys: 344 ms, total: 1.6 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1.2 s\n",
        "('kmeans++/pca data:', 0.45431715023822122, 0.45450800063482932, 0.46690717941807114)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 972 ms, sys: 380 ms, total: 1.35 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 948 ms\n",
        "('kmeans++/pca whiten data:', 0.33072545447083035, 0.33095952972274639, 0.3425093251671309)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sample-wise normalization is very promising to IMAGE DATA, followed by a PCA (ZCA) without whitening**\n",
      "\n",
      "**It seems a whitening (of any kind) is either irrelevant or harmful, specially done with a PCA**\n",
      "\n",
      "**Normalization on a feature-wise basis is harmful to IMAGE data**\n",
      "\n",
      "**Layman rule: LEAVE Features alone, normalize only on samples - it is generally true for kmeans**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TEST Kmeans on 20 news group - TEXT\n",
      "\n",
      "## the feature-wise normalization in NOT really necessary as the difference between\n",
      "## different features are NOT significant\n",
      "\n",
      "## **NOTE by using tfidf, the text data matrix has already been SAMPLEWISE normalized by l2, unless avoid it**\n",
      "## USE RandomizedPCA to deal with large and SPARSE input\n",
      "\n",
      "X = ng_X\n",
      "y = ng_y\n",
      "n_clusters = len(np.unique(y))\n",
      "\n",
      "## feature wise normalization\n",
      "normalized_X = preprocessing.scale(X, axis = 0, with_mean = False)\n",
      "sample_normal_X = preprocessing.normalize(X, norm='l2', axis = 1)\n",
      "pca_X = decomposition.RandomizedPCA(n_components=100, whiten = False).fit_transform(X)\n",
      "pca_whiten_X = decomposition.RandomizedPCA(n_components=100, whiten = True).fit_transform(X)\n",
      "\n",
      "## kmeans++ / raw data\n",
      "%time raw_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0).fit(X).labels_)\n",
      "print('kmeans++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, raw_clusters), \n",
      "      metrics.homogeneity_score(y, raw_clusters),\n",
      "      metrics.completeness_score(y, raw_clusters))\n",
      "## kmeans++ / normalized data\n",
      "%time normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0).fit(normalized_X).labels_)\n",
      "print('kmeans++/normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, normal_clusters), \n",
      "      metrics.homogeneity_score(y, normal_clusters),\n",
      "      metrics.completeness_score(y, normal_clusters))\n",
      "## kmeans++ / sample normalized data\n",
      "%time sample_normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0).fit(sample_normal_X).labels_)\n",
      "print('kmeans++/sample normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, sample_normal_clusters), \n",
      "      metrics.homogeneity_score(y, sample_normal_clusters),\n",
      "      metrics.completeness_score(y, sample_normal_clusters))\n",
      "## pca initialization / raw data\n",
      "pca_components = decomposition.RandomizedPCA(n_components=n_clusters).fit(X).components_\n",
      "%time pcainit_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, init=pca_components, batch_size=1000, random_state=0).fit(X).labels_)\n",
      "print('pca++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pcainit_clusters), \n",
      "      metrics.homogeneity_score(y, pcainit_clusters),\n",
      "      metrics.completeness_score(y, pcainit_clusters))\n",
      "## kmeans++ / pca data\n",
      "%time pca_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0).fit(pca_X).labels_)\n",
      "print('kmeans++/pca data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_clusters), \n",
      "      metrics.homogeneity_score(y, pca_clusters),\n",
      "      metrics.completeness_score(y, pca_clusters))\n",
      "## kmeans++ / pca whiten data\n",
      "%time pca_whiten_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0).fit(pca_whiten_X).labels_)\n",
      "print('kmeans++/pca whiten data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_whiten_clusters), \n",
      "      metrics.homogeneity_score(y, pca_whiten_clusters),\n",
      "      metrics.completeness_score(y, pca_whiten_clusters))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.py:512: DeprecationWarning: Sparse matrix support is deprecated and will be dropped in 0.16. Use TruncatedSVD instead.\n",
        "  DeprecationWarning)\n",
        "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.py:512: DeprecationWarning: Sparse matrix support is deprecated and will be dropped in 0.16. Use TruncatedSVD instead.\n",
        "  DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1.93 s, sys: 8 ms, total: 1.94 s\n",
        "Wall time: 1.94 s\n",
        "('kmeans++/raw data:', 0.0037550691552412528, 0.0074012505314947682, 0.286442107900229)\n",
        "CPU times: user 1.88 s, sys: 0 ns, total: 1.88 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1.88 s\n",
        "('kmeans++/normalized data:', 0.0012398923589927347, 0.0035898422890411337, 0.32401125448308721)\n",
        "CPU times: user 3.5 s, sys: 4 ms, total: 3.5 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 3.5 s\n",
        "('kmeans++/sample normalized data:', 0.2976173491649089, 0.30146145648641404, 0.41439272417924822)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 1.23 s, sys: 36 ms, total: 1.27 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1.23 s\n",
        "('pca++/raw data:', 0.085889513688285846, 0.089667333372454458, 0.23664145156608768)\n",
        "CPU times: user 396 ms, sys: 284 ms, total: 680 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 380 ms\n",
        "('kmeans++/pca data:', 0.074345379517878132, 0.077100987091995907, 0.19645488876715661)\n",
        "CPU times: user 436 ms, sys: 260 ms, total: 696 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 400 ms\n",
        "('kmeans++/pca whiten data:', 0.068546189207856753, 0.071911616744890078, 0.22027548754588058)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/pca.py:512: DeprecationWarning: Sparse matrix support is deprecated and will be dropped in 0.16. Use TruncatedSVD instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Again l2-normalizatoin on a sample basis is very important to KMeans, specially when the dimensionality is very high**\n",
      "\n",
      "**PCA does NOT look to be as useful to text data as to image data, as the noise level is not high**\n",
      "\n",
      "**Low AMI values are a sign of insufficient modelling of data using a small number of clusters**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## TEST Kmeans on 20 news group - blackbox\n",
      "\n",
      "## the feature-wise normalization in NOT really necessary as the difference between\n",
      "## different features are NOT significant\n",
      "\n",
      "## **NOTE by using tfidf, the text data matrix has already been SAMPLEWISE normalized by l2, unless avoid it**\n",
      "## USE RandomizedPCA to deal with large and SPARSE input\n",
      "\n",
      "X = black_X\n",
      "y = black_y\n",
      "n_clusters = len(np.unique(y))\n",
      "\n",
      "## feature wise normalization\n",
      "normalized_X = preprocessing.scale(X, axis = 0, with_mean = False)\n",
      "sample_normal_X = preprocessing.normalize(X, norm='l2', axis = 1)\n",
      "pca_X = decomposition.RandomizedPCA(n_components=50, whiten = False).fit_transform(X)\n",
      "pca_whiten_X = decomposition.RandomizedPCA(n_components=50, whiten = True).fit_transform(X)\n",
      "\n",
      "## kmeans++ / raw data\n",
      "%time raw_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0, max_iter = 200).fit(X).labels_)\n",
      "print('kmeans++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, raw_clusters), \n",
      "      metrics.homogeneity_score(y, raw_clusters),\n",
      "      metrics.completeness_score(y, raw_clusters))\n",
      "## kmeans++ / normalized data\n",
      "%time normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0, max_iter = 200).fit(normalized_X).labels_)\n",
      "print('kmeans++/normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, normal_clusters), \n",
      "      metrics.homogeneity_score(y, normal_clusters),\n",
      "      metrics.completeness_score(y, normal_clusters))\n",
      "## kmeans++ / sample normalized data\n",
      "%time sample_normal_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0, max_iter = 200).fit(sample_normal_X).labels_)\n",
      "print('kmeans++/sample normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, sample_normal_clusters), \n",
      "      metrics.homogeneity_score(y, sample_normal_clusters),\n",
      "      metrics.completeness_score(y, sample_normal_clusters))\n",
      "## pca initialization / raw data\n",
      "pca_components = decomposition.RandomizedPCA(n_components=n_clusters).fit(X).components_\n",
      "%time pcainit_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, init=pca_components, batch_size=1000, random_state=0, max_iter = 200).fit(X).labels_)\n",
      "print('pca++/raw data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pcainit_clusters), \n",
      "      metrics.homogeneity_score(y, pcainit_clusters),\n",
      "      metrics.completeness_score(y, pcainit_clusters))\n",
      "## kmeans++ / pca data\n",
      "%time pca_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0, max_iter = 200).fit(pca_X).labels_)\n",
      "print('kmeans++/pca data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_clusters), \n",
      "      metrics.homogeneity_score(y, pca_clusters),\n",
      "      metrics.completeness_score(y, pca_clusters))\n",
      "## kmeans++ / pca whiten data\n",
      "%time pca_whiten_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=0, max_iter = 200).fit(pca_whiten_X).labels_)\n",
      "print('kmeans++/pca whiten data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pca_whiten_clusters), \n",
      "      metrics.homogeneity_score(y, pca_whiten_clusters),\n",
      "      metrics.completeness_score(y, pca_whiten_clusters))\n",
      "## pca initialization / sample-normalized data\n",
      "pca_components = decomposition.RandomizedPCA(n_components=n_clusters).fit(sample_normal_X).components_\n",
      "%time pcainit_samplenorml_clusters = (cluster.MiniBatchKMeans(n_clusters=n_clusters, init=pca_components, batch_size=1000, random_state=0, max_iter = 200).fit(sample_normal_X).labels_)\n",
      "print('pca++/sample normalized data:', \n",
      "      metrics.adjusted_mutual_info_score(y, pcainit_samplenorml_clusters), \n",
      "      metrics.homogeneity_score(y, pcainit_samplenorml_clusters),\n",
      "      metrics.completeness_score(y, pcainit_samplenorml_clusters))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 4.52 s, sys: 436 ms, total: 4.96 s\n",
        "Wall time: 4.49 s\n",
        "('kmeans++/raw data:', 0.010460922220417886, 0.024760390234169431, 0.025883274302675698)\n",
        "CPU times: user 4.49 s, sys: 368 ms, total: 4.86 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 4.46 s\n",
        "('kmeans++/normalized data:', 0.0078817017315035761, 0.022956465624274641, 0.023937777674144085)\n",
        "CPU times: user 3.14 s, sys: 380 ms, total: 3.52 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 3.12 s\n",
        "('kmeans++/sample normalized data:', 0.0065443097375237104, 0.022083517527491395, 0.023787178285628239)\n",
        "CPU times: user 2.02 s, sys: 116 ms, total: 2.14 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2.01 s\n",
        "('pca++/raw data:', 0.0081874077902406809, 0.023958109664069731, 0.028131646914994635)\n",
        "CPU times: user 88 ms, sys: 68 ms, total: 156 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 77.6 ms\n",
        "('kmeans++/pca data:', 0.0040776946686123137, 0.019514435373986099, 0.020084856913613818)\n",
        "CPU times: user 144 ms, sys: 120 ms, total: 264 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 133 ms\n",
        "('kmeans++/pca whiten data:', 0.0134458481213863, 0.027392044738754437, 0.033502034526481116)\n",
        "CPU times: user 2.28 s, sys: 116 ms, total: 2.39 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2.27 s\n",
        "('pca++/sample normalized data:', 0.003374960398019349, 0.018960751966346376, 0.021516377662362748)\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Low clustering score means the structure has not been captured by such a small number of clusters**\n",
      "\n",
      "**Howevery, the data is quite fat - meaning the number of training examples are not enough to capture the high-dimensional data **\n",
      "\n",
      "\n",
      "**Suprisingly the sample-normalized data does NOT outperform raw data - meaning that the different features are actually VERY CORRELATED - that's why PCA whiten data gives relatively better results**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}