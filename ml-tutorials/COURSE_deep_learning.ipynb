{
 "metadata": {
  "name": "COURSE_deep_learning"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Play with mhorbal's code for deep learning\n",
      "\n",
      "- [code](git clone git@bitbucket.org:mhorbal/learningtools.git)\n",
      "- [host](https://bitbucket.org/mhorbal/learningtools)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Common Use Functions\n",
      "\n",
      "- Functions and Their Derivatives\n",
      "- Functions usually take an np.array as input and output another np.array\n",
      "- Most of the functions are scalar functions (the input is a scalar and the output is a scalar) - use elementwise map only, except softmax (input is a vector and output is a vector) - use map and reduce operator. \n",
      "- Most gradient functions are scalar functions (in a elementwise matrix operator way)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import numpy as np\n",
      "\n",
      "X = np.asarray([\n",
      "        [1, 2],\n",
      "        [3.5, 4],\n",
      "        [5, 6]\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor import nnet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Sigmoid\n",
      "def sigmoid(X):\n",
      "    \"\"\"\n",
      "    numpy.array -> numpy.array\n",
      "    compute sigmoid function: 1 / (1 + exp(-X))\n",
      "    All elememnts should be in [0, 1]\n",
      "    \"\"\"\n",
      "    return 1. / (1. + np.exp(-X))\n",
      "\n",
      "def sigmoid_grad(X):\n",
      "    \"\"\"\n",
      "    It implements element-wise gradient, which takes each element \n",
      "    as a scalar\n",
      "    \"\"\"\n",
      "    sig = sigmoid(X)\n",
      "    return sig * (1 - sig)\n",
      "\n",
      "## theano version - the cost of grad must be a scalar\n",
      "#x = T.matrix()\n",
      "#tsigmoid = theano.function(inputs = [x], outputs = nnet.sigmoid(x))\n",
      "#tsgimoid_grad = theano.function(inputs = [x], outputs = theano.grad(nnet.sigmoid(x), x))\n",
      "\n",
      "print sigmoid(X)\n",
      "print \n",
      "print T.nnet.sigmoid(X).eval()\n",
      "print\n",
      "print sigmoid_grad(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.73105858  0.88079708]\n",
        " [ 0.97068777  0.98201379]\n",
        " [ 0.99330715  0.99752738]]\n",
        "\n",
        "[[ 0.73105858  0.88079708]\n",
        " [ 0.97068777  0.98201379]\n",
        " [ 0.99330715  0.99752738]]\n",
        "\n",
        "[[ 0.19661193  0.10499359]\n",
        " [ 0.02845302  0.01766271]\n",
        " [ 0.00664806  0.00246651]]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## softmax\n",
      "def softmax(X):\n",
      "    \"\"\"\n",
      "    numpy.array -> numpy.array\n",
      "    Compute softmax function: exp(X) / sum(exp(X, 1))\n",
      "    where each row of X is a vector output (e.g., different columns representing \n",
      "    outputs for different classes)\n",
      "    The output of softmax is a matrix, with the sum of each row to be nearly 1.0\n",
      "    as it is the probabilities that are calculated.\n",
      "    \"\"\"\n",
      "    mx = np.max(X)\n",
      "    ex = np.exp(X - mx) # prefer zeros over stack overflow - but NOT always useful\n",
      "    return ex / np.sum(ex, 1).reshape(-1, 1)\n",
      "\n",
      "def softmax_grad(X):\n",
      "    sm = softmax(X)\n",
      "    return sm * (1-sm)\n",
      "\n",
      "print softmax(X)\n",
      "x = T.matrix('x')\n",
      "f = theano.function(inputs = [x], outputs = T.nnet.softmax(x))\n",
      "print \n",
      "print f(X)\n",
      "print \n",
      "print softmax_grad(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.26894142  0.73105858]\n",
        " [ 0.37754067  0.62245933]\n",
        " [ 0.26894142  0.73105858]]\n",
        "\n",
        "[[ 0.26894142  0.73105858]\n",
        " [ 0.37754067  0.62245933]\n",
        " [ 0.26894142  0.73105858]]\n",
        "\n",
        "[[ 0.19661193  0.19661193]\n",
        " [ 0.23500371  0.23500371]\n",
        " [ 0.19661193  0.19661193]]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## tanh and derivatives\n",
      "def tanh(X):\n",
      "    return np.tanh(X)\n",
      "def tanh_grad(X):\n",
      "    return 1. / np.square(np.cosh(X))\n",
      "\n",
      "print tanh(X)\n",
      "print \n",
      "print tanh_grad(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.76159416  0.96402758]\n",
        " [ 0.9981779   0.9993293 ]\n",
        " [ 0.9999092   0.99998771]]\n",
        "\n",
        "[[  4.19974342e-01   7.06508249e-02]\n",
        " [  3.64088472e-03   1.34095068e-03]\n",
        " [  1.81583231e-04   2.45765474e-05]]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## linear and derivatives\n",
      "def linear(X):\n",
      "    return X\n",
      "def linear_grad(X):\n",
      "    return np.ones(X.shape)\n",
      "\n",
      "## rectified linear (ReLU) and derivatives \n",
      "def rectified_linear(X):\n",
      "    return X * (X > 0)\n",
      "def rectified_linear_grad(X):\n",
      "    return (X > 0).astype('b') # return int\n",
      "\n",
      "print linear(X), '\\n'\n",
      "print linear_grad(X), '\\n'\n",
      "print rectified_linear(X), '\\n'\n",
      "print rectified_linear_grad(X), '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.   2. ]\n",
        " [ 3.5  4. ]\n",
        " [ 5.   6. ]] \n",
        "\n",
        "[[ 1.  1.]\n",
        " [ 1.  1.]\n",
        " [ 1.  1.]] \n",
        "\n",
        "[[ 1.   2. ]\n",
        " [ 3.5  4. ]\n",
        " [ 5.   6. ]] \n",
        "\n",
        "[[1 1]\n",
        " [1 1]\n",
        " [1 1]] \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Common Base Class of Learning Models & Helpers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import numpy as np\n",
      "\n",
      "class Model(object):\n",
      "    \"\"\"\n",
      "    Base class for learning models. All model types inheriting this object must include:\n",
      "    - attrs_ class variable: for pretty printing\n",
      "    - cost function: that takes the model and data (and possibly labels) and computes the the cost\n",
      "    of the data given the model. Must return a numeric cost and a gradient.\n",
      "    - train function: that takes the model and data (and possibly labels) and transforms the data \n",
      "    into a suitable format to pass into an optimized algorithm\n",
      "    - update function: that takes the model and gradient to update the model parameters\n",
      "    \"\"\"\n",
      "    attrs_ = []\n",
      "    def __repr__(self):\n",
      "        meat = ', '.join(['%s = %s' % (attr, str(getattr(self, attr))) for attr in self.attrs_])\n",
      "        return self.__class__.__name__ + '(' + meat + ')'\n",
      "    def cost(self, data, target):\n",
      "        raise Error('To be implemented')\n",
      "    def train(self, data, target):\n",
      "        raise Error('To be implemented')\n",
      "    def update(self, grad):\n",
      "        raise Error('To be implemented')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import uniform\n",
      "import numpy as np\n",
      "from functools import partial\n",
      "\n",
      "def initialize_weights(nin, nout):\n",
      "    \"\"\"\n",
      "    weight shape - nout * nin\n",
      "    X shape - nin * nrow\n",
      "    \"\"\"\n",
      "    return (uniform(-1, 1, size = (nout, nin))) / np.sqrt(nin + nout)\n",
      "\n",
      "def compute_numerical_gradient(model, data, target = None, err = 1e-8):\n",
      "    raise Error('TO BE IMPLEMENTED')\n",
      "    \n",
      "    \n",
      "## TESTING\n",
      "print initialize_weights(3, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.05954632 -0.21633934 -0.16199111]\n",
        " [-0.07968537 -0.00524121 -0.07855087]\n",
        " [ 0.10327397  0.03170066  0.02869144]\n",
        " [-0.00843493 -0.01846055  0.12962895]\n",
        " [ 0.19265302 -0.08632744  0.19428462]\n",
        " [ 0.04206315 -0.00778248 -0.1927412 ]\n",
        " [ 0.2437375   0.25630775 -0.19549379]\n",
        " [-0.14286722 -0.02028118  0.10749867]\n",
        " [-0.10868784  0.02095068  0.2106905 ]\n",
        " [ 0.01285801 -0.23510364  0.23589165]]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MLP"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import sparse\n",
      "from numpy.random import uniform\n",
      "\n",
      "MODELFNS = {\n",
      "      'sigmoid': sigmoid\n",
      "    , 'softmax': softmax\n",
      "    , 'tanh': tanh\n",
      "    , 'linear': linear\n",
      "    , 'ReLU': rectified_linear\n",
      "}\n",
      "\n",
      "GRADFNS = {\n",
      "     'sigmoid': sigmoid_grad\n",
      "   , 'softmax': softmax_grad\n",
      "   , 'tanh': tanh_grad\n",
      "   , 'linear': linear_grad\n",
      "   , 'ReLU': rectified_linear_grad\n",
      "}\n",
      "\n",
      "class MLPLayer(Model):\n",
      "    attrs_ = ['nin', 'nout', 'modelfn', 'dropout']\n",
      "    def __init__(self, nin, nout, modelfn = 'sigmoid', dropout = 0.0):\n",
      "        self._W = initialize_weights(nin + 1, nout).astype(np.float)\n",
      "        self._nin = nin\n",
      "        self._nout = nout\n",
      "        self._modelfn = modelfn\n",
      "        self.dropout = dropout\n",
      "    @property\n",
      "    def nin(self):\n",
      "        return self._nin\n",
      "    @property\n",
      "    def nout(self):\n",
      "        return self._nout\n",
      "    @property\n",
      "    def modelfn(self):\n",
      "        return self._modelfn\n",
      "    @property\n",
      "    def W(self):\n",
      "        return self._W\n",
      "    @W.setter\n",
      "    def W(self, value):\n",
      "        self._W = value\n",
      "    @property\n",
      "    def l2_penalty(self):\n",
      "        ## excluding offset b parameter, which is the first COLUMN\n",
      "        return ??\n",
      "    def propup(self, X, ispred = False):\n",
      "        return ??\n",
      "    def backprop(self, ??):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}