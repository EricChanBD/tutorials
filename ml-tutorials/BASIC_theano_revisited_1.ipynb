{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "from theano import function, shared, config\n",
      "import numpy as np\n",
      "import theano.tensor as T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load data\n",
      "train_set, valid_set, test_set = pickle.load(open('data/mnist.pkl'))\n",
      "train_x, train_y = train_set\n",
      "valid_x, valid_y = valid_set\n",
      "test_x, test_y = test_set\n",
      "print train_x.shape, train_y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50000, 784) (50000,)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data_sharing for GPU\n",
      "- When using MSGD, it is encouraged to store the dataset into `shared variables` and access it based on the `minibatch index`, given a fixed and known batch size.\n",
      "- The reason behind shared variables is because of the large overhead when copying data into the GPU memory. \n",
      "- If shared variables are not used, the data would be copied on request (each minibatch individually when needed).\n",
      "- If the data is in theano shared variables, you give theano the possibility to copy the entire data on the GPU in a single call when the shared variables are constructed.\n",
      "- Afterwards, the GPU can access any minibatch by taking a slice from this shared variables, without needing to copy any information from the CPU memory.\n",
      "- Because the data points and their labels are usually of different nature, it is suggested to use different variables for labels and data.\n",
      "- Now the data is in one shared variable, and a minibatch is defined as a slice of that variable, it comes more natural to define a minibatch by indicating its index and its size.\n",
      "- If the memory in GPU is not large enough to fit the data, the code wont run appropriately. However, you can still store sufficiently small chunk of data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def share_data(data, return_type = None, borrow = True):\n",
      "    \"\"\"\n",
      "    Function that loads the dataset into shared variables,\n",
      "    so that theano can copy it into GPU memory.\n",
      "    Since copying data into GPU is slow, copying a minibatch\n",
      "    everytime would lead to a large decrease in performance.\n",
      "    \"\"\"\n",
      "    ## explicitly convert dtype to float32, save the data into GPU\n",
      "    shared_data = theano.shared(np.asarray(data, \n",
      "                                           dtype=theano.config.floatX),\n",
      "                                borrow = borrow)\n",
      "    if return_type:\n",
      "        shared_data = T.cast(shared_data, dtype = return_type)\n",
      "    ## return a reference to GPU copy, with an explicit casting if needed\n",
      "    return shared_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x, train_y = (share_data(train_x), \n",
      "                    share_data(train_y, return_type='int32'))\n",
      "valid_x, valid_y = (share_data(valid_x), \n",
      "                    share_data(valid_y, return_type='int32'))\n",
      "test_x, test_y = (share_data(test_x), \n",
      "                    share_data(test_y, return_type='int32'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mini-batch Stochastic Gradient Descent (MSGD)\n",
      "- The optimizer recommended for deep learning is so-called 'minibatches stochastic gradient descent'.\n",
      "- It is identical to SGD, except that we use more than one training example to make each estimate of the gradient.\n",
      "- This technique reduces variance in the estimate of gradient, and often makes better use of the hierarchical memory organization in modern computers\n",
      "- There is a tradeoff in the choice of the minibatch size B - the reduction of variance and use of SIMD instructionshelps most when increasing B from 1 to 2, but the marginal improvement fades rapidly to nothing. With large B, time is wasted in reducing the variance of the gradient estimator, that time would better spent on additinal gradient steps.\n",
      "- Optimal B is model-, dataset-, and hardware-dependent, can can be anywhere from 1 to maybe several hundreds\n",
      "- The learning result could be sensitive to minibatch size, specially when it is trained for a fixed number of epochs. \n",
      "- Regularization tech: $l1/l2$ regularization or `early-stopping`. In principle, adding a regularization term to the loss will encougrage smooth network mappings in a neural network, by penalizing large values of the parameters, which decreases the amount of nonlinearity that the network models.\n",
      "- Note that the fact that a solution is simple does not mean that it will generalize well. Empiricically, it was found that performing sumch regularization in the context of neural networks helps with generalization, sepcailly on small datasets.\n",
      "- Early-stopping combats overfitting by monitoring the model's performance on a validation set. During the training, if the model's performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, then the heuristic implemented here gives up on much futher optimization.\n",
      "- The choice of when to stop is a judgemental call and a few heuristics exist, but the implementation here will make use of a strategy based on a geometrically increasing amount of patience."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_optimize(n_epochs, learning_rate\n",
      "                 train_cost, train_gradients, train_params):\n",
      "    \"\"\"\n",
      "    Params:\n",
      "    model: the model to be trained, including \n",
      "    \"\"\"\n",
      "    index = T.lscalar()\n",
      "    updates = [(p, p-learning_rate*gp) \n",
      "               for p,gp in zip(train_params, train_gradients)]\n",
      "    train_model = theano.function(inputs = [index],\n",
      "                                  outputs = train_cost, \n",
      "                                  updates = updates)\n",
      "    \n",
      "    best_params = None\n",
      "    best_validation_loss = np.inf\n",
      "    done_looping = False\n",
      "    epoch = 0\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch += 1\n",
      "        \n",
      "        for minibatch_index in xrange(n_batches):\n",
      "            ## training - get cost, \n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            it = (epoch - 1) * n_batches + minibatch_index\n",
      "            \n",
      "            if \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Congjugate Gradient"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression\n",
      "- A probabilistic, linear classifier - classification is done by projecting data points onto aset of hyperplances, the distance to which reflects a class membership probability\n",
      "- Mathematically, it is posterior probability is:\n",
      "$$\n",
      "P(Y=i|x,W,b)=softmax_i(Wx+b)=\\frac{e^{W_ix+b_i}}{\\sum_j{e^{W_jx+b_j}}}\n",
      "$$\n",
      "- A very common cost function for multi-classfication is the *negative log-likelihood*, which is the equivalence of maximizing the likelihood of data set.\n",
      "- The optimization method that is usually used is called `mini-batches stochastic gradient descent (MSGD)`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    def __init__(self, n_in, n_out, \n",
      "                 X = None, y = None, \n",
      "                 W = None, b = None):\n",
      "        self.W = W or shared(value = np.zeros((n_in, n_out),\n",
      "                                        dtype = config.floatX),\n",
      "                               name = 'W', borrow = True)\n",
      "        self.b = b or shared(value = np.zeros((nout, ), \n",
      "                                        dtype = config.floatX),\n",
      "                               name = 'b', borrow = True)\n",
      "        self.X = X or T.matrix('X')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}