{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "from theano import function, shared, config\n",
      "import numpy as np\n",
      "import theano.tensor as T\n",
      "rng = np.random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: Quadro 4000\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load data\n",
      "train_set, valid_set, test_set = pickle.load(open('data/mnist.pkl'))\n",
      "train_x, train_y = train_set\n",
      "valid_x, valid_y = valid_set\n",
      "test_x, test_y = test_set\n",
      "print train_x.shape, train_y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50000, 784) (50000,)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data_sharing for GPU\n",
      "- When using MSGD, it is encouraged to store the dataset into `shared variables` and access it based on the `minibatch index`, given a fixed and known batch size.\n",
      "- The reason behind shared variables is because of the large overhead when copying data into the GPU memory. \n",
      "- If shared variables are not used, the data would be copied on request (each minibatch individually when needed).\n",
      "- If the data is in theano shared variables, you give theano the possibility to copy the entire data on the GPU in a single call when the shared variables are constructed.\n",
      "- Afterwards, the GPU can access any minibatch by taking a slice from this shared variables, without needing to copy any information from the CPU memory.\n",
      "- Because the data points and their labels are usually of different nature, it is suggested to use different variables for labels and data.\n",
      "- Now the data is in one shared variable, and a minibatch is defined as a slice of that variable, it comes more natural to define a minibatch by indicating its index and its size.\n",
      "- If the memory in GPU is not large enough to fit the data, the code wont run appropriately. However, you can still store sufficiently small chunk of data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def share_data(data, return_type = None, borrow = True):\n",
      "    \"\"\"\n",
      "    Function that loads the dataset into shared variables,\n",
      "    so that theano can copy it into GPU memory.\n",
      "    Since copying data into GPU is slow, copying a minibatch\n",
      "    everytime would lead to a large decrease in performance.\n",
      "    \"\"\"\n",
      "    ## explicitly convert dtype to float32, save the data into GPU\n",
      "    shared_data = shared(np.asarray(data, \n",
      "                                           dtype=config.floatX),\n",
      "                                borrow = borrow)\n",
      "    if return_type:\n",
      "        shared_data = T.cast(shared_data, dtype = return_type)\n",
      "    ## return a reference to GPU copy, with an explicit casting if needed\n",
      "    return shared_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mini-batch Stochastic Gradient Descent (MSGD)\n",
      "- The optimizer recommended for deep learning is so-called 'minibatches stochastic gradient descent'.\n",
      "- It is identical to SGD, except that we use more than one training example to make each estimate of the gradient.\n",
      "- This technique reduces variance in the estimate of gradient, and often makes better use of the hierarchical memory organization in modern computers\n",
      "- There is a tradeoff in the choice of the minibatch size B - the reduction of variance and use of SIMD instructionshelps most when increasing B from 1 to 2, but the marginal improvement fades rapidly to nothing. With large B, time is wasted in reducing the variance of the gradient estimator, that time would better spent on additinal gradient steps.\n",
      "- Optimal B is model-, dataset-, and hardware-dependent, can can be anywhere from 1 to maybe several hundreds\n",
      "- The learning result could be sensitive to minibatch size, specially when it is trained for a fixed number of epochs. \n",
      "- Regularization tech: $l1/l2$ regularization or `early-stopping`. In principle, adding a regularization term to the loss will encougrage smooth network mappings in a neural network, by penalizing large values of the parameters, which decreases the amount of nonlinearity that the network models.\n",
      "- Note that the fact that a solution is simple does not mean that it will generalize well. Empiricically, it was found that performing sumch regularization in the context of neural networks helps with generalization, sepcailly on small datasets.\n",
      "- Early-stopping combats overfitting by monitoring the model's performance on a validation set. During the training, if the model's performance ceases to improve sufficiently on the validation set, or even degrades with further optimization, then the heuristic implemented here gives up on much futher optimization.\n",
      "- The choice of when to stop is a judgemental call and a few heuristics exist, but the implementation here will make use of a strategy based on a geometrically increasing amount of patience."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SGDOptimizer(object):\n",
      "    def __init__(self, \n",
      "                 n_train_batches,\n",
      "                 n_valid_batches,\n",
      "                 verbose = True, \n",
      "                 patience = 5000, \n",
      "                 patience_increase = 2, \n",
      "                 improvement_threshold = 0.995):\n",
      "        self.n_train_batches = n_train_batches\n",
      "        self.n_valid_batches = n_valid_batches\n",
      "        self.verbose = verbose\n",
      "        self.patience = patience\n",
      "        self.patience_increase = patience_increase\n",
      "        self.improvement_threshold = improvement_threshold\n",
      "    def optimize(self, n_epochs, train_fn, valid_fn, params):\n",
      "        validation_frequency = min(self.n_train_batches, self.patience / 2)\n",
      "        self.best_params = None\n",
      "        self.best_validation_error = np.inf\n",
      "        \n",
      "        epoch = 0\n",
      "        out_of_patience = False\n",
      "        patience = self.patience\n",
      "        while (epoch < n_epochs) and (not out_of_patience):\n",
      "            epoch += 1\n",
      "            for minibatch_index in xrange(n_train_batches):\n",
      "                train_cost = train_fn(minibatch_index)\n",
      "                niter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "                if (niter + 1) % validation_frequency == 0:\n",
      "                    validation_error = np.mean([validate_fn(i) \n",
      "                                                for i in xrange(self.n_valid_batches)])\n",
      "                    if self.verbose:\n",
      "                        print 'epoch %i, minibatch %i/%i, validation error %f' % (\n",
      "                            epoch, minibatch_index + 1, self.n_train_batches,\n",
      "                            validation_error\n",
      "                        )\n",
      "                    if validation_error < self.best_validation_error:\n",
      "                        self.best_validation_error = validation_error\n",
      "                        self.best_params = [p.get_value(borrow = True) for p in params]\n",
      "                        if validation_error < self.best_validation_error * self.improvement_threshold:\n",
      "                            patience = max(self.patience, niter * self.patience_increase)\n",
      "                if patience <= niter:\n",
      "                    out_of_patience = True\n",
      "                    print 'running out of patience ...'\n",
      "                    break\n",
      "        if self.verbose:\n",
      "            print 'optimization done with best validation: %f' % self.best_validation_error\n",
      "        return self.best_params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Congjugate Gradient"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression\n",
      "- A probabilistic, linear classifier - classification is done by projecting data points onto aset of hyperplances, the distance to which reflects a class membership probability\n",
      "- Mathematically, it is posterior probability is:\n",
      "$$\n",
      "P(Y=i|x,W,b)=softmax_i(Wx+b)=\\frac{e^{W_ix+b_i}}{\\sum_j{e^{W_jx+b_j}}}\n",
      "$$\n",
      "- A very common cost function for multi-classfication is the *negative log-likelihood*, which is the equivalence of maximizing the likelihood of data set.\n",
      "- The optimization method that is usually used is called `mini-batches stochastic gradient descent (MSGD)`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### implementation trick:\n",
      "\n",
      "1. define complete set of indepedent variables\n",
      "2. define dependent variables and their relationship with independent variables\n",
      "3. define functions that only use those variables (with any explicit logic anymore)\n",
      "4. to change the actual value of a variable during the function exeution:\n",
      "    - put the variable as function inputs - not applicable for shared variables\n",
      "    - use set_value of shared variables - only applicable for shared variables\n",
      "    - use givens parameter - applicable for both non-shared and shared variables, but the replacement will still be T.variables, instead of numpy values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    def __init__(self, n_in, n_out, \n",
      "                 X = None, y = None, \n",
      "                 W = None, b = None):\n",
      "        \"\"\"\n",
      "        params here are everything that you need for calculations.\n",
      "        It should define the relationship graph of all the variables.\n",
      "        So that later functions can be defined by using those variables.\n",
      "        \"\"\"\n",
      "        self.n_in = n_in\n",
      "        self.n_out = n_out\n",
      "        self.W = W or shared(value = np.zeros((n_in, n_out),\n",
      "                                        dtype = config.floatX),\n",
      "                               name = 'W', borrow = True)\n",
      "        self.b = b or shared(value = np.zeros((n_out, ), \n",
      "                                        dtype = config.floatX),\n",
      "                               name = 'b', borrow = True)\n",
      "        self.X = X or T.matrix('X')\n",
      "        self.y = y or T.ivector('y')\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(self.X, self.W) + self.b)\n",
      "        self.y_hat = T.argmax(self.p_y_given_x, axis = 1)\n",
      "        self.nll = -T.mean(T.log(self.p_y_given_x[T.arange(self.y.shape[0]), self.y]))\n",
      "        self.classification_rate = T.mean(T.eq(self.y_hat, self.y))\n",
      "    def get_cost_fn(self, data_x, data_y, batch_size, learning_rate = 0.13):\n",
      "        \"\"\"\n",
      "        bind data to the variables of the formula,\n",
      "        return a function with sig: minibatch_cost cost_f(minibatch_index),\n",
      "        where the cost is calculated \n",
      "        \"\"\"\n",
      "        shared_x = share_data(data_x)\n",
      "        shared_y = share_data(data_y, return_type = 'int32')\n",
      "        index = T.lscalar('index')\n",
      "        gw, gb = T.grad(self.nll, wrt = (self.W, self.b))\n",
      "        updates = [(self.W, self.W-gw), (self.b, self.b-gb)]\n",
      "        return function(inputs = [index], \n",
      "                        outputs = self.nll, \n",
      "                        updates = updates,\n",
      "                        givens = {self.X: shared_x[index*batch_size:(index+1)*batch_size],\n",
      "                                  self.y: shared_y[index*batch_size:(index+1)*batch_size]})\n",
      "    def get_score_fn(self, X, y):\n",
      "        shared_x = share_data(X)\n",
      "        shared_y = share_data(y, return_type = 'int32')\n",
      "        return function(inputs = [], \n",
      "                        outputs = [self.classification_rate, self.nll],\n",
      "                        givens = {self.X: shared_x, self.y: shared_y})\n",
      "    def bind_params(self, W_value, b_value):\n",
      "        self.W.set_value(W_value)\n",
      "        self.b.set_value(b_value)\n",
      "        return self"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## test sgd with logistic regression\n",
      "n_train, n_in = train_x.shape\n",
      "n_valid, n_in = valid_x.shape\n",
      "batch_size = 600\n",
      "n_train_batches = n_train / batch_size\n",
      "n_valid_batches = n_valid / batch_size\n",
      "n_out = 10\n",
      "\n",
      "lr = LogisticRegression(n_in, n_out)\n",
      "train_fn = lr.get_cost_fn(train_x, train_y, batch_size)\n",
      "validate_fn = lr.get_cost_fn(valid_x, valid_y, batch_size)\n",
      "\n",
      "sgd = SGDOptimizer(n_train_batches, n_valid_batches)\n",
      "optimal_w, optimal_b = sgd.optimize(1000, train_fn, validate_fn, params = (lr.W, lr.b))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 1, minibatch 83/83, validation error 0.337476\n",
        "epoch 2, minibatch 83/83, validation error 0.304742"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 83/83, validation error 0.289617"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 83/83, validation error 0.280311"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 83/83, validation error 0.273784"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 83/83, validation error 0.268838"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 83/83, validation error 0.264895"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 83/83, validation error 0.261639"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 83/83, validation error 0.258879"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 83/83, validation error 0.256493"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 83/83, validation error 0.254398"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 83/83, validation error 0.252536"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 83/83, validation error 0.250863"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 83/83, validation error 0.249348"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 83/83, validation error 0.247966"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 83/83, validation error 0.246697"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 83/83, validation error 0.245527"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 83/83, validation error 0.244441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 83/83, validation error 0.243430"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 83/83, validation error 0.242486"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 83/83, validation error 0.241600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 83/83, validation error 0.240768"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 83/83, validation error 0.239982"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 83/83, validation error 0.239240"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 83/83, validation error 0.238537"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 83/83, validation error 0.237870"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 83/83, validation error 0.237235"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 83/83, validation error 0.236630"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 83/83, validation error 0.236052"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 83/83, validation error 0.235500"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 83/83, validation error 0.234972"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 83/83, validation error 0.234465"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 83/83, validation error 0.233979"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 83/83, validation error 0.233512"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 83/83, validation error 0.233062"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 83/83, validation error 0.232630"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 83/83, validation error 0.232212"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 83/83, validation error 0.231809"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 83/83, validation error 0.231420"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 83/83, validation error 0.231044"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 83/83, validation error 0.230680"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 83/83, validation error 0.230328"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 83/83, validation error 0.229986"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 83/83, validation error 0.229655"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 83/83, validation error 0.229334"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 83/83, validation error 0.229022"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 83/83, validation error 0.228719"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 83/83, validation error 0.228424"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 83/83, validation error 0.228137"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 83/83, validation error 0.227857"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 83/83, validation error 0.227585"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 83/83, validation error 0.227320"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 83/83, validation error 0.227062"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 83/83, validation error 0.226810"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 83/83, validation error 0.226564"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 83/83, validation error 0.226324"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 83/83, validation error 0.226089"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 58, minibatch 83/83, validation error 0.225860"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 59, minibatch 83/83, validation error 0.225635"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 83/83, validation error 0.225416"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "running out of patience ...\n",
        "optimization done with best validation: 0.225416\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.bind_params(optimal_w, optimal_b)\n",
      "score_valid = lr.get_score_fn(valid_x, valid_y)\n",
      "score_test = lr.get_score_fn(test_x, test_y)\n",
      "print score_valid()\n",
      "print score_test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array(0.9429), array(0.21705274283885956, dtype=float32)]\n",
        "[array(0.9243), array(0.27065593004226685, dtype=float32)]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MLP\n",
      "- the implementation uses a MLP with one single layer - because if it is more than that, the back-propagation does NOT work really well.\n",
      "- the hidden layer of MLP can be viewed as several logistic regression running indepedently, and the ouput layer is another logistic regression on top of that\n",
      "- conceputally, the input is first transformed using a learnt non-linear transformation, the purpose of the transformation is to porject the input data into a space where it becomes linearly separable.\n",
      "- a single hidden layer is sufficient to make MLPs a universal approximator. However there are substantial benefits to using many such hidden layers. \n",
      "- activation functions\n",
      "    - typical nonlinear transformation for hidden layers are `tanh` and `sigmoid`, such as \n",
      "        $$tanh(a) = \\frac{e^a-e^{-a}}{e^a+e^{-a}}$$\n",
      "        $$sigmoid(a) = \\frac{1}{1+e^{-a}}$$\n",
      "    - both $tanh$ and $sigmoid$ are scalar-to-scalar functions but their natural extension to vectors and tensors consists in applying them elementwise\n",
      "    - the output range of $tanh$ is [-1, 1] and [0, 1] for $sigmoid$. so sigmoid is like a streched version of tahn\n",
      "    - $tanh$ usually yields to faster training an sometimes also to better local minima\n",
      "    - the good initializations for weights using different activations are different:\n",
      "        - $$tanh: [-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}, \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}]$$\n",
      "        - $$sigmoid: [-4\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}, 4\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}]$$\n",
      "      those defines a regime of an activation function where information can easily be propagated both upward (activations flowing from inputs to outputs) and backward (gradients flowing from outputs to inputs)\n",
      "- as usual, the softmax was chosen for output layer to appoximate the posterior probability"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## drawing tanh and sigmoid\n",
      "tanh = lambda a: (e**a - e**(-a)) / (e**a + e**(-a))\n",
      "sigmoid = lambda a: 1. / (1 + e**(-a))\n",
      "xs = np.arange(-10, 10, 0.5)\n",
      "figure()\n",
      "plot(xs, [tanh(x) for x in xs], label='$tanh$', color = 'r')\n",
      "plot(xs, [sigmoid(x) for x in xs], label = '$sigmoid$', color = 'b')\n",
      "legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<matplotlib.legend.Legend at 0x8112490>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD9CAYAAABQvqc9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFMMcEgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x786a390>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HiddenLayer(object):\n",
      "    def __init__(self, n_in, n_out,\n",
      "                 X = None, \n",
      "                 W = None, b = None, \n",
      "                 activation = T.tanh):\n",
      "        self.n_in = n_in\n",
      "        self.n_out = n_out\n",
      "        self.X = X or T.matrix('X')\n",
      "        if W:\n",
      "            self.W = W\n",
      "        else:\n",
      "            W_values = (rng.uniform(low = -np.sqrt(6. / (n_in + n_out)), \n",
      "                                    high = np.sqrt(6. / (n_in + n_out)), \n",
      "                                    size = (n_in, n_out))\n",
      "                            .astype(config.floatX))\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_values = W_values * 4 \n",
      "            self.W = shared(value = W_values, name = 'W', borrow = True)\n",
      "        if b:\n",
      "            self.b = b\n",
      "        else:\n",
      "            b_values = np.zeros((n_out, ), dtype=config.floatX)\n",
      "            self.b = shared(value = b_values, name = 'b', borrow = True)\n",
      "        self.lin_output = T.dot(self.X, self.W) + self.b\n",
      "        self.output = (self.lin_output if activation is None \n",
      "                       else activation(self.lin_output))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## mlp implementation\n",
      "class MLP(object):\n",
      "    def __init__(self, n_in, n_out, n_hidden,\n",
      "                 X = None, y = None,\n",
      "                 activation = T.tanh):\n",
      "        self.n_in = n_in\n",
      "        self.n_out = n_out\n",
      "        self.n_hidden = n_hidden\n",
      "        self.X = X or T.matrix('X')\n",
      "        self.y = y or T.imatrix('y')\n",
      "        self.hidden_layer = HiddenLayer(n_in, n_hidden, self.X)\n",
      "        self.output_layer = LogisticRegression(n_hidden, n_out, \n",
      "                                               self.hidden_layer.output, self.y)\n",
      "        self.params = (self.hidden_layer.W, self.hidden_layer.b, \n",
      "                       self.output_layer.W, self.output_layer.b)\n",
      "        ## norm penalty only on W parts, not b parts\n",
      "        self.L1 = T.sum(abs(self.hidden_layer.W)) + T.sum(abs(self.output_layer.W))\n",
      "        self.L2 = T.sum(self.hidden_layer.W**2) + T.sum(self.output_layer.W**2)\n",
      "        self.nll = self.output_layer.nll\n",
      "    def get_cost_fn(self, data_x, data_y, \n",
      "                    l1_alpha = 0.001, l2_alpha = 0.001,\n",
      "                    batch_size = 20, learning_rate = 0.01):\n",
      "        shared_x = share_data(data_x)\n",
      "        shared_y = share_data(data_y, return_type = 'int32')\n",
      "        minibatch_index = T.lmatrix('index')\n",
      "        cost = self.nll + l1_alpha * self.L1 + l2_alpha * self.L2\n",
      "        gparams = T.grad(cost, wrt = self.params)\n",
      "        updates = [(p, p - learning_rate * gp) for p, gp in zip(self.params, gparams)]\n",
      "        return function(inputs = [minibatch_index], \n",
      "                        outputs = cost, \n",
      "                        updates = updates, \n",
      "                        givens = {\n",
      "                            self.X: shared_x[minibatch_index],\n",
      "                            self.y: shared_y[minibatch_index]\n",
      "                        })"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## test mlp \n",
      "n_train, n_in = train_x.shape\n",
      "n_valid, n_in = valid_x.shape\n",
      "batch_size = 600\n",
      "n_train_batches = n_train / batch_size\n",
      "n_valid_batches = n_valid / batch_size\n",
      "n_hidden = 500\n",
      "n_out = 10\n",
      "\n",
      "mlp = MLP(n_in, n_out, n_hidden)\n",
      "train_fn = mlp.get_cost_fn(train_x, train_y)\n",
      "validate_fn = lr.get_cost_fn(valid_x, valid_y)\n",
      "\n",
      "sgd = SGDOptimizer(n_train_batches, n_valid_batches)\n",
      "optimal_w, optimal_b = sgd.optimize(1000, train_fn, validate_fn, params = mlp.params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NotImplementedError",
       "evalue": "Could not import inplace_increment, so some advanced indexing features are disabled. They will be available if you update NumPy to version 1.8 or later, or to the latest development version.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-0dae02fad1a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cost_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mvalidate_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cost_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-10-065e7dce52da>\u001b[0m in \u001b[0;36mget_cost_fn\u001b[1;34m(self, data_x, data_y, l1_alpha, l2_alpha, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mminibatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml1_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml2_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         return function(inputs = [minibatch_index], \n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     rval = _populate_grad_dict(var_to_app_to_idx,\n\u001b[1;32m--> 528\u001b[1;33m             grad_dict, wrt, cost_name)\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36m_populate_grad_dict\u001b[1;34m(var_to_app_to_idx, grad_dict, wrt, cost_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwrt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    922\u001b[0m                                 str(g_shape))\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m                 \u001b[0minput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_output_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minput_grads\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/tensor/subtensor.pyc\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(self, inputs, grads)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1881\u001b[0m         return [advanced_inc_subtensor(theano.tensor.zeros_like(x), gz,\n\u001b[1;32m-> 1882\u001b[1;33m                                        *rest)] + \\\n\u001b[0m\u001b[0;32m   1883\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mDisconnectedType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m    386\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_stack_trace_on_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/tensor/subtensor.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, x, y, *inputs)\u001b[0m\n\u001b[0;32m   1950\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1951\u001b[0m                 raise NotImplementedError(\n\u001b[1;32m-> 1952\u001b[1;33m                         \u001b[1;34m'Could not import inplace_increment, so some advanced '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1953\u001b[0m                         \u001b[1;34m'indexing features are disabled. They will be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m                         \u001b[1;34m'available if you update NumPy to version 1.8 or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNotImplementedError\u001b[0m: Could not import inplace_increment, so some advanced indexing features are disabled. They will be available if you update NumPy to version 1.8 or later, or to the latest development version."
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T.grad(mlp.nll, mlp.hidden_layer.W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NotImplementedError",
       "evalue": "Could not import inplace_increment, so some advanced indexing features are disabled. They will be available if you update NumPy to version 1.8 or later, or to the latest development version.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-35-f7fe93ebd6cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     rval = _populate_grad_dict(var_to_app_to_idx,\n\u001b[1;32m--> 528\u001b[1;33m             grad_dict, wrt, cost_name)\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36m_populate_grad_dict\u001b[1;34m(var_to_app_to_idx, grad_dict, wrt, cost_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwrt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m             \u001b[0moutput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccess_grad_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[1;31m# list of bools indicating if each output is connected to the cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_grad_cache\u001b[1;34m(var)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccess_term_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gradient.pyc\u001b[0m in \u001b[0;36maccess_term_cache\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    922\u001b[0m                                 str(g_shape))\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m                 \u001b[0minput_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_output_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minput_grads\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/tensor/subtensor.pyc\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(self, inputs, grads)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1881\u001b[0m         return [advanced_inc_subtensor(theano.tensor.zeros_like(x), gz,\n\u001b[1;32m-> 1882\u001b[1;33m                                        *rest)] + \\\n\u001b[0m\u001b[0;32m   1883\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mDisconnectedType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m    386\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_stack_trace_on_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.6.0rc3-py2.7.egg/theano/tensor/subtensor.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, x, y, *inputs)\u001b[0m\n\u001b[0;32m   1950\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1951\u001b[0m                 raise NotImplementedError(\n\u001b[1;32m-> 1952\u001b[1;33m                         \u001b[1;34m'Could not import inplace_increment, so some advanced '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1953\u001b[0m                         \u001b[1;34m'indexing features are disabled. They will be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m                         \u001b[1;34m'available if you update NumPy to version 1.8 or '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNotImplementedError\u001b[0m: Could not import inplace_increment, so some advanced indexing features are disabled. They will be available if you update NumPy to version 1.8 or later, or to the latest development version."
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}