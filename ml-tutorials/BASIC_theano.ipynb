{
 "metadata": {
  "name": "BASIC_theano"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic Tutorial for Theano deep learning package"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function, shared\n",
      "from theano import pp\n",
      "import theano\n",
      "import numpy as np\n",
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Basics of Tensor Functionality***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## create symbols of variables and functions\n",
      "## 1. use T.xx to directly create variables\n",
      "## 2. use operators (such as +) to create new variables - TensorVariable\n",
      "## 3. use function to create function symbols\n",
      "x = T.dscalar('x') ## ALL SYMBOLS must be typed, T.dscalar = 0-d arrays (scalar) of doubles (d)\n",
      "print type(x) ## dscalar is similiar to dtype in np, so x is not an instance of dscalar, but TensorVariable\n",
      "y = T.dscalar('y') ## T.dscalar() is like a factory method\n",
      "print y.type is T.dscalar ## the \"dtype\" of a TensorVariable is accessed by 'type' attr \n",
      "z = x + y\n",
      "print type(z), pp(z) ## pp can be used for pretty-print of variables, but for functions\n",
      "f = function([x, y], z) ## compiling function object to C code, f CAN be used like a normal python function\n",
      "print type(f), f ## cannot use pp for f\n",
      "print f(10.1, 100)\n",
      "print z.eval({x: 10.1, y: 100}) ## z's eval() is equivalent to function(), but less flexible"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.basic.TensorVariable'>\n",
        "True\n",
        "<class 'theano.tensor.basic.TensorVariable'> (x + y)\n",
        "<class 'theano.compile.function_module.Function'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <theano.compile.function_module.Function object at 0x10b918bd0>\n",
        "110.1\n",
        "110.1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## every symbol in theano must be typed, which can be done by using specific factory method in T\n",
      "## matrices algebra\n",
      "x = T.dmatrix('x') # matrix of double- matrices are DUCK-typed, np.array or list of list, it RETURNS np.array\n",
      "print type(x), x.type, pp(x)\n",
      "y = T.dmatrix('y')\n",
      "z = x * y ## elementwise multiplication\n",
      "f = function([x, y], z)\n",
      "## perform f on list of list\n",
      "r = f([[1, 2], [3, 4]], [[10, 20], [30, 40]])\n",
      "print type(r)\n",
      "print r\n",
      "## perform f again on np.array\n",
      "r = f(np.asarray([[1, 2], [3, 4]]), np.asarray([[10, 20], [30, 40]]))\n",
      "print type(r)\n",
      "print r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.basic.TensorVariable'> TensorType(float64, matrix) x\n",
        "<type 'numpy.ndarray'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[  10.   40.]\n",
        " [  90.  160.]]\n",
        "<type 'numpy.ndarray'>\n",
        "[[  10.   40.]\n",
        " [  90.  160.]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Theano symbol typing system*** \n",
      "\n",
      "- **byte**: bscalar, bvector, bmatrix, brow, bcol, btensor3, btensor4\n",
      "- **16-bit integers**: wscalar, wvector, wmatrix, wrow, wcol, wtensor3, wtensor4\n",
      "- **32-bit integers**: iscalar, ivector, imatrix, irow, icol, itensor3, itensor4\n",
      "- **64-bit integers**: lscalar, lvector, lmatrix, lrow, lcol, ltensor3, ltensor4\n",
      "- **float**: fscalar, fvector, fmatrix, frow, fcol, ftensor3, ftensor4\n",
      "- **double**: dscalar, dvector, dmatrix, drow, dcol, dtensor3, dtensor4\n",
      "- **complex**: cscalar, cvector, cmatrix, crow, ccol, ctensor3, ctensor4\n",
      "\n",
      "**system: scalar, vector, matrix, row, col, tensor3, tensor4**\n",
      "\n",
      "they can be created using the specific type-specific factory methods, or just using the *common* factory methods, \n",
      "such as tensor.scalar(name, dtype=config.floatX)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## shared TensorVariable\n",
      "## Shared TensorVariables are usually used to convert existing python objects to symbol variables\n",
      "## it is more like another type of beast, which is why shared is now in theano pkg directly\n",
      "## instead of in theano.tensor\n",
      "x = shared(np.random.randn(3, 4))\n",
      "print type(x), pp(x), x.type\n",
      "print x.value\n",
      "print T.shape(x)\n",
      "## use eval to get the value in the shared variable\n",
      "print x.eval()\n",
      "## the created variable is a TensorSharedVariable whose .value is a np array in this case\n",
      "## the dtype of x is inferred from the ndarray"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.sharedvar.TensorSharedVariable'> <TensorType(float64, matrix)> TensorType(float64, matrix)\n",
        "(<property object at 0x109aa5e68>,)\n",
        "Shape.0\n",
        "[[ 0.97698495 -1.53368543  0.92842832  0.64082965]\n",
        " [-1.12715379  0.0224872   0.49577723 -0.99701666]\n",
        " [ 0.14669318  1.45799837 -0.38410862  0.23779664]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*The argument to shared variable will NOT be copied, and subsequent changes will be reflected in X.value*\n",
      "\n",
      "*On the other hande, theano makes a copy of any ndarray that you use in an expression, so subsequent changes to that ndarray will not have any effect on the theano expression*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Types of TensorVariables***\n",
      "\n",
      "- *TensorVariable*: The result of symbolic operations typically have this type.\n",
      "- *TensorConstant*: *Python and numpy numbers* are wrapped in this type.\n",
      "- *TensorSharedVariable*: This type is returned by shared() when the value to share is a *numpy ndarray*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***TensorVariable Operation***\n",
      "\n",
      "Tensor pacakage has different types of operators, which can be used to combine different types of variables to create new tensor variables, such as \n",
      "\n",
      "- tensor.reshape, tensor.shape\n",
      "- tensor.flatten\n",
      "- tensor.zeros_like\n",
      "- tensor.stack\n",
      "- tensor.concatenate\n",
      "- tensor.sum\n",
      "- other operators such as indexing\n",
      "- tensor.lt, tensor.gt, and etc\n",
      "- tensor.switch\n",
      "- tensor.dot, tensor.outer\n",
      "- tensor.grad\n",
      "\n",
      "<a href=\"http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-basic-tensor\">Documentation</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***All about defining functions***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## define function that returns MULTIPLE OUTPUT\n",
      "a, b = T.dmatrices('a', 'b')\n",
      "diff = a - b ## - operator\n",
      "abs_diff = abs(a-b) ## abs operator\n",
      "diff_squared = diff ** 2 ## ** operator\n",
      "f = function([a, b], [diff, abs_diff, diff_squared])\n",
      "pprint(f([[0, 1], [-1, -2]], [[1, 1], [-1, -2]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[-1.,  0.],\n",
        "       [ 0.,  0.]]),\n",
        " array([[ 1.,  0.],\n",
        "       [ 0.,  0.]]),\n",
        " array([[ 1.,  0.],\n",
        "       [ 0.,  0.]])]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## setting a DEFAULT value for an argument\n",
      "## Use theano.Param to WRAP a variable and a default value\n",
      "from theano import Param # Param, Shared are in theano package\n",
      "## Param class allows you to specify properties of your functions\n",
      "## parameters with greater details, such as mutability, default and etc.\n",
      "## Inputs with default values must follow inputs without default values.\n",
      "## Param even allows the overridding of variable name for the specific function.\n",
      "x, y = T.dmatrices('x', 'y')\n",
      "z = x + y\n",
      "f = function([x, Param(y, default=[[1, 2]], name='y_by_name')], z)\n",
      "pprint(f([[2, 3]]))\n",
      "pprint(f([[2, 3]], y_by_name=[[0, 0]])) # use the override name to specify named params\n",
      "pprint(f([[2, 3]], [[0, 0]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "array([[ 3.,  5.]])\n",
        "array([[ 2.,  3.]])\n",
        "array([[ 2.,  3.]])\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Write a function with internal state - using Shared variable\n",
      "## example: accumulator\n",
      "from theano import shared\n",
      "state = shared(0)\n",
      "inc = T.iscalar('inc')\n",
      "accumulator = function([inc], state, updates = [(state, state+inc)]) # inputs, outputs, updates\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(1)\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(10)\n",
      "state.set_value(0) ## reset\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(1)\n",
      "\n",
      "## Shared variables are hybrid symbolic and non-symbolic variables whose\n",
      "## value may be shared between multiple functions.\n",
      "## Shared variables can be used in symbolic expressions just like the objects\n",
      "## returned by dmatrices(...) but they also have an internal value that defines\n",
      "## the value taken by this symbolic variable in ALL the functions that use it.\n",
      "\n",
      "## The value stored in the shared variable can be accessed and modified by the \n",
      "## .get_value() and .set_value() methods.\n",
      "\n",
      "## the updates parameter of a Function is a list of pairs of the form \n",
      "## (shared_variable, new_expression) or a dictionary\n",
      "\n",
      "## Main reasons for using shared variable is their efficiency."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shared state: 0\n",
        "0\n",
        "shared state: 1\n",
        "1\n",
        "shared state: 0\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Introducing RANDOMNESS in functions\n",
      "## IN theano a random number generator is implemented as a Random Stream Object\n",
      "## Random Streams are at their core SHARED variables. \n",
      "## Theano random objects are defined and implemented in RandomStreams and at a lower level\n",
      "## RandomStreamBase.\n",
      "## MORE ON RANDOMNESS http://deeplearning.net/software/theano/tutorial/examples.html\n",
      "from theano.tensor.shared_randomstreams import RandomStreams # REMEMBER THIS!\n",
      "## RANDOM STREAM\n",
      "srng = RandomStreams(seed = 0) ## random generator (stream)\n",
      "print type(srng)\n",
      "## RANDOM VARIABLES\n",
      "rv_u = srng.uniform((2, 2)) # random variable, which will update rv_u.rng state everytime\n",
      "rv_n = srng.normal((2, 2))\n",
      "print type(rv_u), rv_u.type, rv_u.rng\n",
      "f = function([], rv_u) ## update srng every time - different values every time\n",
      "g = function([], rv_n, no_default_updates=True) # NOT UPDATING srng - same value every time\n",
      "print f()\n",
      "print f()\n",
      "print g()\n",
      "print g()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.shared_randomstreams.RandomStreams'>\n",
        "<class 'theano.tensor.basic.TensorVariable'> TensorType(float64, matrix) <RandomStateType>\n",
        "[[ 0.48604732  0.68571232]\n",
        " [ 0.98557605  0.19559641]]\n",
        "[[ 0.58341167  0.98058218]\n",
        " [ 0.1804803   0.70146864]]\n",
        "[[ 1.99759307  0.35128336]\n",
        " [ 1.50384112  1.25808594]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 1.99759307  0.35128336]\n",
        " [ 1.50384112  1.25808594]]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Example of Logistic Regression***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Computing logistic function - elementwise way\n",
      "## definition 1: logistic(x) = 1. / (1. + exp(-x))\n",
      "x = T.dmatrix('x')\n",
      "s = 1 / (1 + T.exp(-x))\n",
      "logistic = function([x], s)\n",
      "print logistic([[0, 1], [-1, -2]])\n",
      "## definition 2: logistic(x) = (1. + tanh(x/2)) / 2\n",
      "s2 = (1 + T.tanh(x/2)) / 2\n",
      "logistic2 = function([x], s2)\n",
      "print logistic2([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.5         0.73105858]\n",
        " [ 0.26894142  0.11920292]]\n",
        "[[ 0.5         0.73105858]\n",
        " [ 0.26894142  0.11920292]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Logistic Regression\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = np.random # random number generator\n",
      "\n",
      "## DATA\n",
      "N = 400\n",
      "feats = 1000\n",
      "data_X, data_y = (rng.randn(N, feats), rng.randint(size = N, low = 0, high = 2))\n",
      "training_steps = 10000\n",
      "print data_X.shape, data_y.shape\n",
      "print np.unique(data_y)\n",
      "\n",
      "## theano symbolic variables\n",
      "x = T.dmatrix('x')\n",
      "y = T.dvector('y') ## y is A VECTOR!!!\n",
      "w = theano.shared(rng.randn(feats), name='w')\n",
      "b = theano.shared(0., name='b')\n",
      "#print 'initial model:'\n",
      "#print w.get_value(), b.get_value()\n",
      "\n",
      "## theano expression graph\n",
      "p_1 = 1 / (1 + T.exp(- T.dot(x, w) - b))\n",
      "prediction = p_1 > 0.5\n",
      "x_ent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)\n",
      "cost = T.mean(x_ent) + 0.01 * T.sum(abs(w)) # cost = xent.mean() + 0.01 * (w ** 2).sum()\n",
      "accuracy = T.mean(T.eq(prediction, y))\n",
      "gw, gb = T.grad(cost, [w, b])\n",
      "\n",
      "## compile - shared w, b for train and predict function\n",
      "train = theano.function(\n",
      "            inputs = [x, y],\n",
      "            outputs = [prediction, x_ent],\n",
      "            updates = {w: w - 0.1*gw, b: b-0.1*gb})\n",
      "predict = theano.function(inputs = [x], outputs = prediction)\n",
      "score = theano.function(inputs = [x, y], outputs = accuracy)\n",
      "\n",
      "## train\n",
      "for i in xrange(training_steps):\n",
      "    if i % 1000 == 0:\n",
      "        print 'iteration ', i \n",
      "    pred, err = train(data_X, data_y)\n",
      "    \n",
      "## predict\n",
      "print 'performance on D:', score(data_X, data_y)\n",
      "print 'significant number of features:', sum(np.abs(w.get_value()) > 0) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(400, 1000) (400,)\n",
        "[0 1]\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "performance on D:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0\n",
        "significant number of features: 1000\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Wrap theano logistic regression as sklearn model\n",
      "from sklearn.base import BaseEstimator\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "class LogisticRegression(BaseEstimator):\n",
      "    def __init__(self, alpha = 0.01, n_iters = 10000, learning_rate = 0.1):\n",
      "        ## meta params\n",
      "        self.rng = np.random\n",
      "        self.alpha = alpha\n",
      "        self.n_iters = n_iters\n",
      "        self.learning_rate = 0.1\n",
      "        ## independent symbols\n",
      "        X = T.dmatrix('X')\n",
      "        y = T.dvector('y')\n",
      "        self.w = theano.shared(rng.randn(1), 'w') # we dont know the dim yet\n",
      "        self.b = theano.shared(0., 'b')\n",
      "        ## dependent expressions\n",
      "        p_1 = 1 / (1 + T.exp(- T.dot(X, self.w) - self.b))\n",
      "        prediction = p_1 > 0.5\n",
      "        x_ent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)\n",
      "        cost = T.mean(x_ent) + self.alpha * T.sum(self.w**2)\n",
      "        accuracy = T.mean(T.eq(y, prediction))\n",
      "        gw, gb = T.grad(cost, [self.w, self.b])\n",
      "        ## functions\n",
      "        self.train = theano.function(\n",
      "                        inputs = [X, y],\n",
      "                        outputs = [prediction, x_ent],\n",
      "                        updates = ((self.w, self.w - self.learning_rate * gw),\n",
      "                                   (self.b, self.b - self.learning_rate * gb)))\n",
      "        self.predict = theano.function(\n",
      "                        inputs = [X],\n",
      "                        outputs = prediction)\n",
      "        self.predict_prob = theano.function(\n",
      "                        inputs = [X],\n",
      "                        outputs = p_1)\n",
      "        self.score = theano.function(\n",
      "                        inputs = [X, y],\n",
      "                        outputs = accuracy)\n",
      "    def fit(self, X, y):\n",
      "        ## intialize w, and b values\n",
      "        n_samples, n_feats = X.shape\n",
      "        self.w.set_value(self.rng.randn(n_feats))\n",
      "        ## train on self.w and self.b\n",
      "        for i in xrange(self.n_iters):\n",
      "            if i % 1000 == 0:\n",
      "                print 'iteration:', i\n",
      "            self.train(X, y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        return self.predict(X)\n",
      "    def predict_prob(self, X):\n",
      "        return self.predict_prob(X)\n",
      "    def score(self, X, y):\n",
      "        return self.score(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "from sklearn.cross_validation import train_test_split\n",
      "data_X, data_y = cPickle.load(open('data/blackbox.pkl', 'rb'))\n",
      "data_y[data_y != 1] = 0 # binary classification\n",
      "train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size = 0.2)\n",
      "\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(train_X, train_y)\n",
      "print lr.score(test_X, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "0.77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## parallel run\n",
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "dv = client[:]\n",
      "print 'runing on ', len(dv)\n",
      "dv.block = True\n",
      "dv['train_X'] = train_X\n",
      "dv['train_y'] = train_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "runing on  4\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%load 80\n",
      "lr = LogisticRegression()\n",
      "lr.fit(train_X, train_y)\n",
      "print lr.score(train_X, train_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "0.81625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Flow of Control***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## CONDITIONING\n",
      "## two main ops: theano.ifelse.ifelse or T.switch\n",
      "## ifelse => takes a boolean condition and two variables, lazy eval of one variable (returned)\n",
      "## switch => takes a tensor as condition and two variables. It is eleementwise and more general\n",
      "\n",
      "## NOTE: Unless linker='vm' or linker='cvm' are used, \n",
      "## ifelse will compute both variables and take the same computation time as switch. \n",
      "## Although the linker is not currently set by default to cvm, it will be in the near future\n",
      "from theano.ifelse import ifelse\n",
      "import time\n",
      "a, b = T.scalars('a', 'b')\n",
      "x, y = T.matrices('x', 'y')\n",
      "\n",
      "z_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y)) # evaluate both means of x and y\n",
      "z_lazy = ifelse(T.lt(a, b), T.mean(x), T.mean(y)) # evaluate one of them\n",
      "\n",
      "f_switch = theano.function([a, b, x, y], z_switch, mode = theano.Mode(linker = 'vm'))\n",
      "f_lazyifelse = theano.function([a, b, x, y], z_lazy, mode = theano.Mode(linker = 'vm'))\n",
      "\n",
      "val1, val2 = 0., 1.\n",
      "big_mat1, big_mat2 = np.ones((10000, 1000)), np.ones((10000, 1000))\n",
      "\n",
      "%timeit f_switch(val1, val2, big_mat1, big_mat2)\n",
      "%timeit f_lazyifelse(val1, val2, big_mat1, big_mat2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 22.2 ms per loop\n",
        "100 loops, best of 3: 10.9 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## LOOPS\n",
      "## The most general way of doing loop in theano is through the scan op\n",
      "## Both reduction and map can be viewed as special cases of scan\n",
      "## How it works:\n",
      "## The op scans a function along some input sequence, producing an output at each time-step\n",
      "## the function can see the previous K time steps of the function\n",
      "## Unchanging variables are passed to scan as non_sequences. Initialization occurs\n",
      "## in outputs_info. And the accumulation happens automatically\n",
      "## The general order of function parameter to fn param in scan is:\n",
      "## sequences (if any), prior result(s) (if needed), non-sequences (if any)\n",
      "\n",
      "## ELEMENTWISE POWER\n",
      "k = T.iscalar('k')\n",
      "A = T.vector('A') ## float\n",
      "def inner_fct(prior_result, B):\n",
      "    return prior_result * B\n",
      "## Symbolic description of the result - specially define the UPDATES steps\n",
      "result, updates = theano.scan(fn = inner_fct, outputs_info=T.ones_like(A), \n",
      "                            non_sequences=A, n_steps = k)\n",
      "## Scan has provided us with A ** 1 through A ** k.  Keep only the last\n",
      "## value. Scan notices this and does not waste memory saving them.\n",
      "final_result = result[-1]\n",
      "power = theano.function(inputs = [A, k], outputs = final_result, updates = updates)\n",
      "print power(range(10), 2)\n",
      "\n",
      "## POLYNOMIAL\n",
      "coefficents = T.vector('coefficients')\n",
      "x = T.scalar('x')\n",
      "## symbolic representation of polynomial components and updates\n",
      "## Be careful with the initial prior T.constant(0.0), dtype need to be \n",
      "## provided as coefficents.dtype otherwise a downcasting error will happen\n",
      "results, updates = theano.scan(fn = lambda coef, power, prior, x: (power+1, prior+coef*(x**power)),\n",
      "                                    outputs_info = [0., T.as_tensor_variable(np.asarray(0., coefficents.dtype))],\n",
      "                                                        #T.constant(0.0, dtype=coefficents.dtype)],\n",
      "                                    sequences = coefficents,\n",
      "                                    non_sequences = x)\n",
      "result = results[1][-1] # results = [seq(power), seq(sum_of_components)]\n",
      "poly = theano.function(inputs = [coefficents, x], outputs = result, updates = updates)\n",
      "print poly([1., 2, 3], 2)\n",
      "print sum([2 ** i * c for (i, c) in enumerate([1, 2, 3])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
        "17.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## YET ANOTHER POLYNOMIAL\n",
      "power_coeff_pairs = T.matrix('power_coeff_pairs')\n",
      "x = T.scalar('x')\n",
      "results, updates = theano.scan(fn = lambda power_coeff, x: power_coeff[1] * (x ** power_coeff[0]),\n",
      "                                outputs_info = None,\n",
      "                                sequences = power_coeff_pairs,\n",
      "                                non_sequences = x)\n",
      "result = T.sum(results)\n",
      "poly = theano.function(inputs = [power_coeff_pairs, x], outputs = result, updates = updates)\n",
      "print poly(list(enumerate([1, 2, 3])), 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17.0\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T.arange(1, 10).eval()\n",
      "T.as_tensor_variable?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Sparse Matrices in Theano***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Theano sparse matrices are based on scipy sparse package, \n",
      "## it currently supports two types, namely csc and csr formats, for fast linear algebra\n",
      "## A general rule of choosing between csr and csc formats is:\n",
      "## If shape[0] > shape[1], use csr format. Otherwise, use csc.\n",
      "## ANOTHER ONE is: Use the format compatible with the ops in your computation graph.\n",
      "import scipy.sparse as sp\n",
      "from theano import sparse\n",
      "print sparse.all_dtypes\n",
      "\n",
      "## MOVE FROM and TO dense matrices\n",
      "x = sparse.csc_matrix('x')\n",
      "print x.type\n",
      "print sparse.dense_from_sparse(x).type\n",
      "print sparse.csr_from_dense(sparse.dense_from_sparse(x)).type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['uint64', 'int32', 'int16', 'complex128', 'complex64', 'float64', 'uint8', 'uint32', 'uint16', 'int64', 'int8', 'float32'])\n",
        "Sparse[float64, csc]\n",
        "TensorType(float64, matrix)\n",
        "Sparse[float64, csr]\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Logistic Regression for Multiple Classification with SGD***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## based on tutorials at\n",
      "## https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/logistic_sgd.py\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import time\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    \"\"\"\n",
      "    It looks like a pure symoblic model, without any real data \n",
      "    involved. \n",
      "    IS IT A GOOD PRACTICE IN THEANO TO SEPERATE SYMBOLIC REPRESENTATION FROM REAL DATA??\n",
      "    AND EVEN FURTHER SEPERATED FROM OPTIMIZER MODEL\n",
      "    It plays the role of backend support - creating functions that can be directly applied\n",
      "    onto data\n",
      "    \"\"\"\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        ## symbolic variables\n",
      "        self.W = theano.shared(value = np.zeros((n_in, n_out)), name = 'W', borrow = True)\n",
      "        self.b = theano.shared(value = np.zeros((n_out,)), name = 'b', borrow = True)\n",
      "        ## expressions\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "    def negative_log_likelihood(self, y):\n",
      "        \"\"\"\n",
      "        y = theano.tensor.TensorType\n",
      "        \"\"\"\n",
      "        ## dim shuffling \n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "    def errors(self, y):\n",
      "        return T.mean(T.neq(y, self.y_pred))\n",
      "    \n",
      "def shared_dataset(data_xy, borrow = True):\n",
      "    \"\"\"\n",
      "    Function that loads the dataset into shared variables.\n",
      "\n",
      "    The reason we store our dataset in shared variables is to allow\n",
      "    Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "    Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "    is needed (the default behavior if the data is not in a shared variable)\n",
      "    would lead to a large decrease in performance\n",
      "    \"\"\"\n",
      "    data_x, data_y = data_xy\n",
      "    shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX),\n",
      "                            borrow = borrow)\n",
      "    shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX),\n",
      "                            borrow = borrow)\n",
      "    ## when storing data on the GPU it has to be stored as floats \n",
      "    ## therefore we will store the labels as 'floatX' as well. But\n",
      "    ## during our computations we need them as ints (we use labels as index).\n",
      "    ## therefore insetead of returning shared_y we will have to cast it into int.\n",
      "    ## this little hack lets us get around this issue\n",
      "    return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "def sgd_optimize(train_data, test_data, \n",
      "        learning_rate = 0.13, n_epochs = 1000, batch_size = 600):\n",
      "    \"\"\"\n",
      "    Demostrate stochastic gradient descent optimization for a log-linear model\n",
      "    \"\"\"\n",
      "    train_X, train_y = train_data\n",
      "    test_X, test_y = test_data\n",
      "    n_samples, n_feats = train_X.shape.eval()\n",
      "    n_train_batches = train_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    n_test_batches = test_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    print 'building the model ...'\n",
      "    \n",
      "    index = T.lscalar() # index of minibatch\n",
      "    XX = T.matrix('X')\n",
      "    yy = T.ivector('y')\n",
      "    classifier = LogisticRegression(input = XX, n_in = n_feats, n_out = 10)\n",
      "    cost = classifier.negative_log_likelihood(yy)\n",
      "    test_model = theano.function(inputs = [index],\n",
      "                    outputs = classifier.errors(yy),\n",
      "                    givens = {\n",
      "                      XX: test_X[index * batch_size: (index+1) * batch_size],\n",
      "                      yy: test_y[index * batch_size: (index+1) * batch_size]\n",
      "                    })\n",
      "    g_W = T.grad(cost = cost, wrt = classifier.W)\n",
      "    g_b = T.grad(cost = cost, wrt = classifier.b)\n",
      "    updates = [(classifier.W, classifier.W - learning_rate * g_W), \n",
      "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
      "    train_model = theano.function(inputs = [index],\n",
      "                    outputs=cost,\n",
      "                    updates = updates,\n",
      "                    givens = {\n",
      "                      XX: train_X[index * batch_size: (index+1) * batch_size],\n",
      "                      yy: train_y[index * batch_size: (index+1) * batch_size]\n",
      "                    })\n",
      "    print '... training the model'\n",
      "    ## early stopping parameters\n",
      "    ## look as this many examples regardless\n",
      "    patience = 5000 \n",
      "    ## wait this much longer when a new best is found\n",
      "    patience_increase = 2 \n",
      "    ## a relative improvement of this much is considered signifcant\n",
      "    improvement_threshold = 0.995 \n",
      "    ## go through this many minibatches before checking the network\n",
      "    ## on the test set; in this case we check every epoch\n",
      "    validation_frequency = min(n_train_batches, patience/2)\n",
      "    \n",
      "    best_params = None\n",
      "    best_validation_loss = np.inf\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "    \n",
      "    done_looping = False\n",
      "    epoch = 0\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            ## iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                ## do the validation - compute zero-one on test set\n",
      "                validation_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
      "                this_validation_loss = np.mean(validation_losses)\n",
      "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
      "                 epoch, minibatch_index+1, n_train_batches, this_validation_loss*100.\n",
      "                ))\n",
      "                ## if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    ## improve patience if loss improvement is good enough\n",
      "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "                    best_validation_loss = this_validation_loss\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "    end_time = time.clock()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load data\n",
      "import cPickle\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X, y = cPickle.load(open('data/digits.pkl', 'rb'))\n",
      "print X.shape, y.shape\n",
      "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n",
      "(train_X, train_y) = shared_dataset((train_X, train_y))\n",
      "(test_X, test_y) = shared_dataset((test_X, test_y))\n",
      "\n",
      "sgd_optimize((train_X, train_y), (test_X, test_y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(42000, 784) (42000,)\n",
        "building the model ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... training the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 56/56, validation error 17.130952 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 56/56, validation error 18.250000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 56/56, validation error 11.464286 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 56/56, validation error 31.107143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 56/56, validation error 21.357143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 56/56, validation error 19.202381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 56/56, validation error 21.726190 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 56/56, validation error 30.107143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 56/56, validation error 18.869048 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 56/56, validation error 27.523810 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 56/56, validation error 9.321429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 56/56, validation error 19.702381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 56/56, validation error 9.523810 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 56/56, validation error 19.619048 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 56/56, validation error 9.357143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 56/56, validation error 29.011905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 56/56, validation error 9.035714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 56/56, validation error 20.071429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 56/56, validation error 19.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 56/56, validation error 9.190476 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 56/56, validation error 8.571429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 56/56, validation error 9.404762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 56/56, validation error 21.809524 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 56/56, validation error 19.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 56/56, validation error 8.952381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 56/56, validation error 21.166667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 56/56, validation error 8.785714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 56/56, validation error 8.797619 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 56/56, validation error 10.869048 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 56/56, validation error 10.202381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 56/56, validation error 19.821429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 56/56, validation error 29.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 56/56, validation error 10.988095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 56/56, validation error 10.428571 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 56/56, validation error 18.535714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 56/56, validation error 9.488095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 56/56, validation error 11.035714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 56/56, validation error 18.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 56/56, validation error 8.916667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 56/56, validation error 9.119048 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 56/56, validation error 11.880952 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 56/56, validation error 19.142857 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 56/56, validation error 20.809524 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 56/56, validation error 20.011905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 56/56, validation error 9.166667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 56/56, validation error 8.797619 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 56/56, validation error 19.285714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 56/56, validation error 9.892857 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 56/56, validation error 9.119048 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 56/56, validation error 21.952381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 56/56, validation error 9.261905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 56/56, validation error 9.976190 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 56/56, validation error 9.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 56/56, validation error 9.726190 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 56/56, validation error 9.821429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 56/56, validation error 22.488095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 56/56, validation error 9.238095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 58, minibatch 56/56, validation error 12.476190 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 59, minibatch 56/56, validation error 9.095238 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 60, minibatch 56/56, validation error 9.190476 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 61, minibatch 56/56, validation error 9.261905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 62, minibatch 56/56, validation error 8.833333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 63, minibatch 56/56, validation error 9.750000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 64, minibatch 56/56, validation error 19.130952 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 65, minibatch 56/56, validation error 19.095238 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 66, minibatch 56/56, validation error 17.273810 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 67, minibatch 56/56, validation error 9.202381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 68, minibatch 56/56, validation error 11.476190 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 69, minibatch 56/56, validation error 10.035714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 70, minibatch 56/56, validation error 9.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 71, minibatch 56/56, validation error 9.297619 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 72, minibatch 56/56, validation error 8.785714 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 73, minibatch 56/56, validation error 10.773810 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 74, minibatch 56/56, validation error 15.642857 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 75, minibatch 56/56, validation error 8.940476 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 76, minibatch 56/56, validation error 9.380952 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 77, minibatch 56/56, validation error 9.595238 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 78, minibatch 56/56, validation error 8.761905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 79, minibatch 56/56, validation error 20.011905 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 80, minibatch 56/56, validation error 9.416667 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 81, minibatch 56/56, validation error 21.571429 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 82, minibatch 56/56, validation error 19.333333 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 83, minibatch 56/56, validation error 9.023810 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 84, minibatch 56/56, validation error 10.452381 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 85, minibatch 56/56, validation error 9.595238 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 86, minibatch 56/56, validation error 10.607143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 87, minibatch 56/56, validation error 9.250000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 88, minibatch 56/56, validation error 9.940476 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 89, minibatch 56/56, validation error 9.488095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Multiclass Logistic Regression with CG***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        self.theta = theano.shared(value = np.zeros(n_in*n_out+n_out,\n",
      "                                    dtype = theano.config.floatX),\n",
      "                        name = 'theta',\n",
      "                        borrow = True)\n",
      "        self.W = self.theta[:n_in*n_out].reshape((n_in, n_out))\n",
      "        self.b = self.theta[n_in*n_out:]\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W)+self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "    def errors(self, y):\n",
      "        return T.mean(T.neq(self.y_pred, y))\n",
      "    \n",
      "def share_data(raw_data, dtype=theano.config.floatX):\n",
      "    shared_data = theano.shared(value = np.asarray(raw_data, \n",
      "                                dtype=theano.config.floatX),\n",
      "                borrow = True)\n",
      "    return T.cast(shared_data, dtype=dtype)\n",
      "\n",
      "def cg_optimize(v_train_X, v_train_y, v_validation_X, v_validation_y,\n",
      "                    batch_size = 600, n_epoches = 50):\n",
      "    ishape = (28, 28)\n",
      "    n_in = 28 * 28\n",
      "    n_out = 10\n",
      "    \n",
      "    n_train_batches = v_train_X.get_value(borrow=True).shape[0]  / batch_size\n",
      "    n_validation_batches = v_validation_X.get_value(borrow=True).shape[0] / batch_size\n",
      "    \n",
      "    print 'building the model ...'\n",
      "    index = T.lscalar()\n",
      "    x = T.matrix()\n",
      "    y = T.ivector()\n",
      "    classifier = LogisticRegression(input = x, n_in = n_in, n_out = n_out)\n",
      "    cost = classifier.negative_log_likelihood(y)\n",
      "    validate_model = theano.function(inputs = [index],\n",
      "                        outputs = classifier.errors(y),\n",
      "                        givens = {\n",
      "                          x: v_validation_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_validation_y[index*batch_size:(index+1)*batch_size]\n",
      "                        })\n",
      "    batch_cost = theano.function(inputs = [index],\n",
      "                        outputs = cost,\n",
      "                        givens = {\n",
      "                          x: v_train_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_train_y[index*batch_size:(index+1)*batch_size]\n",
      "                        })\n",
      "    batch_grad = theano.function(inputs = [index],\n",
      "                        outputs = T.grad(cost, classifier.theta),\n",
      "                        givens = {\n",
      "                          x: v_train_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_train_y[index*batch_size:(index+1)*batch_size]\n",
      "                        })\n",
      "    ## helper function for scipy optimization\n",
      "    best_validation_score = np.inf\n",
      "    \n",
      "    def train_fn(theta_value):\n",
      "        classifier.theta.set_value(theta_value, borrow = True)\n",
      "        train_loss = np.mean([batch_cost(i) for i in xrange(n_train_batches)])\n",
      "        return train_loss\n",
      "    def train_fn_grad(theta_value):\n",
      "        classifier.theta.set_value(theta_value, borrow = True)\n",
      "        #grad = batch_grad(0)\n",
      "        grad = sum([batch_grad(i) for i in xrange(n_train_batches)]) / n_train_batches\n",
      "        return grad\n",
      "    def callback(theta_value):\n",
      "        classifier.theta.set_value(theta_value, borrow = True)\n",
      "        ## compute the validation loss\n",
      "        validation_loss = np.mean([validate_model(i) for i in xrange(n_validation_batches)])\n",
      "        #print 'validation error %f %%' % validation_loss * 100.\n",
      "        print 'validation error', validation_loss\n",
      "        #if validation_loss < best_validation_score:\n",
      "        #    best_validation_score = validation_loss\n",
      "            \n",
      "    import scipy.optimize\n",
      "    print 'Optimizing using scipy.optimize.fmin_cg...'\n",
      "    start_time = time.clock()\n",
      "    best_w_b = scipy.optimize.fmin_cg(\n",
      "          f = train_fn,\n",
      "          x0 = np.zeros((n_in+1)*n_out, dtype=x.dtype),\n",
      "          fprime = train_fn_grad,\n",
      "          callback = callback,\n",
      "          disp = 0,\n",
      "          maxiter = n_epoches\n",
      "    )\n",
      "    end_time = time.clock()\n",
      "    print best_w_b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load digits data\n",
      "X, y = cPickle.load(open('data/digits.pkl', 'rb'))\n",
      "print X.shape, y.shape\n",
      "print np.unique(y)\n",
      "train_X, validation_X, train_y, validation_y = train_test_split(X, y, test_size = 0.2)\n",
      "v_train_X, v_validation_X = share_data(train_X), share_data(validation_X)\n",
      "v_train_y, v_validation_y = share_data(train_y, dtype='int32'), share_data(validation_y, dtype='int32')\n",
      "cg_optimize(v_train_X, v_train_y, v_validation_X, v_validation_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(42000, 784) (42000,)\n",
        "[0 1 2 3 4 5 6 7 8 9]\n",
        "building the model ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Optimizing using scipy.optimize.fmin_cg..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.33130952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.212142857143\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.169880952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.166666666667\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.149047619048\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.137261904762\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.131666666667\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.122142857143\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.117619047619\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.116785714286\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.11630952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.113928571429\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.110714285714\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.104880952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.101666666667\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0985714285714\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0983333333333\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0969047619048\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0953571428571\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.095\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0960714285714\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0954761904762\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0939285714286\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0947619047619\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0938095238095\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0927380952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.092619047619\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0905952380952\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0890476190476\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0908333333333\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.09\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0891666666667\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0890476190476\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0888095238095\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0879761904762\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0886904761905\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0883333333333\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0890476190476\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0895238095238\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0884523809524\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0869047619048\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0875\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0857142857143\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0878571428571\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0872619047619\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.084880952381\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0855952380952\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0865476190476\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0866666666667\n",
        "validation error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.085\n",
        "[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   1.60823719e-05\n",
        "  -4.80044308e-05  -7.06488857e-06]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***MLPClassification***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    ## binding with input\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        self.W = theano.shared(value = np.zeros((n_in, n_out), \n",
      "                                    dtype = theano.config.floatX),\n",
      "                                name = 'W', borrow = True)\n",
      "        self.b = theano.shared(value = np.zeros((n_out, ), \n",
      "                                    dtype = theano.config.floatX),\n",
      "                                name = 'b', borrow = True)\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
      "        self.params = [self.W, self.b]\n",
      "    ## binding with y\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "    def errors(self, y):\n",
      "        return T.mean(T.neq(self.y_pred, y))\n",
      "    \n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation=T.tanh):\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_value = np.asarray(rng.uniform(\n",
      "                            low = -np.sqrt(6. / (n_in + n_out)), \n",
      "                            high = np.sqrt(6. / (n_in + n_out)), \n",
      "                            size = (n_in, n_out)), \n",
      "                        dtype = theano.config.floatX)\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_value *= 4\n",
      "            W = theano.shared(value = W_value, name = 'W', borrow = True)\n",
      "        if b is None:\n",
      "            b_value = np.zeros((n_out, ), dtype = theano.config.floatX)\n",
      "            b = theano.shared(value = b_value, name = 'b', borrow = True)\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = lin_output if activation is None else activation(lin_output)\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class MLP(object):\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        self.hiddenlayer = HiddenLayer(rng = rng, input = input, n_in = n_in,\n",
      "                                n_out = n_hidden, activation = T.tanh)\n",
      "        self.logRegressioinLayer = LogisticRegression(input = self.hiddenlayer.output, \n",
      "                                n_in = n_hidden, n_out = n_out)\n",
      "        ## L1 norm\n",
      "        self.L1 = abs(self.hiddenlayer.W).sum() + abs(self.logRegressioinLayer.W).sum()\n",
      "        ## L2 norm\n",
      "        self.L2_sqr = (self.hiddenlayer.W ** 2).sum() + (self.logRegressioinLayer.W**2).sum()\n",
      "        self.negative_log_likelihood = self.logRegressioinLayer.negative_log_likelihood\n",
      "        self.errors = self.logRegressioinLayer.errors\n",
      "        self.params = self.hiddenlayer.params + self.logRegressioinLayer.params\n",
      "        \n",
      "        \n",
      "def share_data(data, dtype = theano.config.floatX):\n",
      "    shared_data = theano.shared(np.asarray(data, dtype=theano.config.floatX), \n",
      "                                    borrow = True)\n",
      "    return T.cast(shared_data, dtype = dtype)\n",
      "\n",
      "def run_mlp(v_train_X, v_train_y, v_validation_X, v_validation_y, \n",
      "                learning_rate = 0.01, L1_reg = 0.00, L2_reg = 0.0001, n_epochs = 1000,\n",
      "                batch_size = 20, n_hidden = 500):\n",
      "    n_train_batches = v_train_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    n_validation_batches = v_validation_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    print '... building the model'\n",
      "    \n",
      "    ## symoblic variables\n",
      "    index = T.lscalar()\n",
      "    x = T.matrix('x')\n",
      "    y = T.ivector('y')\n",
      "    rng = np.random.RandomState(0)\n",
      "    \n",
      "    classifier = MLP(rng = rng, input = x, n_in = 28 * 28, n_hidden = n_hidden, n_out=10)\n",
      "    cost = classifier.negative_log_likelihood(y) + L1_reg * classifier.L1 + L2_reg * classifier.L2_sqr\n",
      "    validate_model = theano.function(inputs = [index],\n",
      "                        outputs = classifier.errors(y),\n",
      "                        givens = {\n",
      "                          x: v_validation_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_validation_y[index*batch_size:(index+1)*batch_size]\n",
      "                        })\n",
      "    gparams = T.grad(cost, classifier.params)\n",
      "    updates = [(param, param-learning_rate*gparam) for (param, gparam) in zip(classifier.params, gparams)]\n",
      "    train_model = theano.function(inputs = [index],\n",
      "                        outputs = cost,\n",
      "                        updates = updates,\n",
      "                        givens = {\n",
      "                          x: v_train_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_train_y[index*batch_size:(index+1)*batch_size]\n",
      "                        })\n",
      "    print '... training'\n",
      "    patience = 10000\n",
      "    patience_increase = 2\n",
      "    improvement_threshold = 0.995\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "    best_params = None\n",
      "    best_validation_loss = np.inf\n",
      "    start_time = time.clock()\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "            ## if it is time, check the validataion\n",
      "            if (iter+1) % validation_frequency == 0:\n",
      "                validation_loss = np.mean([validate_model(i) for i in xrange(n_validation_batches)])\n",
      "                print 'epoch %i, minibatch %i / %i, validation error %f %%' % (epoch, minibatch_index+1, \n",
      "                        n_train_batches, validation_loss * 100.)\n",
      "                if validation_loss < best_validation_loss:\n",
      "                    if validation_loss < best_validation_loss * improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "                    best_validation_loss = validation_loss\n",
      "                    best_params = classifier.params\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "    end_time = time.clock()\n",
      "    print 'Optimization Complete. Best Validation score of %f %%' % (best_validation_loss * 100.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load digits data\n",
      "import cPickle\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X, y = cPickle.load(open('data/digits.pkl', 'rb'))\n",
      "print X.shape, y.shape\n",
      "print np.unique(y)\n",
      "train_X, validation_X, train_y, validation_y = train_test_split(X, y, test_size = 0.2)\n",
      "v_train_X, v_validation_X = share_data(train_X), share_data(validation_X)\n",
      "v_train_y, v_validation_y = share_data(train_y, dtype='int32'), share_data(validation_y, dtype='int32')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(42000, 784) (42000,)\n",
        "[0 1 2 3 4 5 6 7 8 9]\n",
        "... building the model"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "... training"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "global name 'valiation_loss' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-0da74d5f7f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mv_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mv_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-9-c99b1f43f020>\u001b[0m in \u001b[0;36mrun_mlp\u001b[0;34m(v_train_X, v_train_y, v_validation_X, v_validation_y, learning_rate, L1_reg, L2_reg, n_epochs, batch_size, n_hidden)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_validation_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 print 'epoch %i, minibatch %i / %i, validation error %f %%' % (epoch, minibatch_index+1, \n\u001b[0;32m--> 114\u001b[0;31m                         n_train_batches, valiation_loss * 100.)\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_validation_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimprovement_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'valiation_loss' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_mlp(v_train_X, v_train_y, v_validation_X, v_validation_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 1680 / 1680, validation error 10.880952 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 1680 / 1680, validation error 10.607143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 1680 / 1680, validation error 10.357143 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 1680 / 1680, validation error 9.845238 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 1680 / 1680, validation error 10.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 1680 / 1680, validation error 9.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 1680 / 1680, validation error 9.892857 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 1680 / 1680, validation error 9.797619 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 1680 / 1680, validation error 9.738095 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 1680 / 1680, validation error 9.964286 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 1680 / 1680, validation error 9.964286 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Optimization Complete. Best Validation score of 9.154762 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Convolutionary Neural Network (LeNet5)***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T\n",
      "import time\n",
      "import numpy as np\n",
      "from theano.tensor.signal import downsample\n",
      "from theano.tensor.nnet import conv\n",
      "\n",
      "class LeNetConvPoolLayer(object):\n",
      "    \"\"\"\n",
      "    Pool Layer of a convolutional network\n",
      "    \"\"\"\n",
      "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n",
      "        \"\"\"\n",
      "        Allocate a LeNetConvPoolLayer with Shared variable internal params\n",
      "        rng = np.random.RandomState\n",
      "        input = theano.tensor.dtensor4, symbolic image tensor\n",
      "        filter_shape = tuple of list of length 4,\n",
      "        (n_filters, n_input_feats_maps, filter_height, filter_width)\n",
      "        image_shape = (batch_size, n_input_feats_maps, img_height, img_width)\n",
      "        poolsize = the downsampling (pooling) factor (n_rows, n_cols)\n",
      "        \"\"\"\n",
      "        assert image_shape[1] == filter_shape[1]\n",
      "        self.input = input\n",
      "        ## there are n_input_feats_maps * n_filter_height * n_filter_width\n",
      "        ## inputs to each hidden unit\n",
      "        fan_in = np.prod(filter_shape[1:])\n",
      "        ## each unit in the lower layer receives a gradient from:\n",
      "        ## n_output_feats_maps * filter_height * filter_width / pooling_size\n",
      "        fan_out = (filter_shape[0] * np.prod(filter_shape[2:]) / np.prod(poolsize))\n",
      "        ## initialize weights\n",
      "        W_bound = np.sqrt(6. / (fan_in + fan_out))\n",
      "        self.W = theano.shared(np.asarray(rng.uniform(\n",
      "                                        low = -W_bound,\n",
      "                                        high = W_bound,\n",
      "                                        size = filter_shape), \n",
      "                                        dtype=theano.config.floatX),\n",
      "                                borrow = True)\n",
      "        b_values = np.zeros((filter_shape[0], ), dtype = theano.config.floatX)\n",
      "        self.b = theano.shared(value = b_values, borrow = True)\n",
      "        ## convolve input feature maps with filters\n",
      "        conv_out = conv.conv2d(input = input, filters = self.W, \n",
      "                            filter_shape = filter_shape, image_shape = image_shape)\n",
      "        ## downsample each feature map individually, using maxpooling\n",
      "        pooled_out = downsample.max_pool_2d(input = conv_out, \n",
      "                            ds = poolsize, ignore_border = True)\n",
      "        ## add the bias term, but first recast into a tensor shape of\n",
      "        ## (1, n_filters, 1, 1)\n",
      "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "class LogisticRegression(object):\n",
      "    ## binding with input\n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        self.W = theano.shared(value = np.zeros((n_in, n_out), \n",
      "                                    dtype = theano.config.floatX),\n",
      "                                name = 'W', borrow = True)\n",
      "        self.b = theano.shared(value = np.zeros((n_out, ), \n",
      "                                    dtype = theano.config.floatX),\n",
      "                                name = 'b', borrow = True)\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis = 1)\n",
      "        self.params = [self.W, self.b]\n",
      "    ## binding with y\n",
      "    def negative_log_likelihood(self, y):\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "    def errors(self, y):\n",
      "        return T.mean(T.neq(self.y_pred, y))\n",
      "    \n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W = None, b = None, activation=T.tanh):\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_value = np.asarray(rng.uniform(\n",
      "                            low = -np.sqrt(6. / (n_in + n_out)), \n",
      "                            high = np.sqrt(6. / (n_in + n_out)), \n",
      "                            size = (n_in, n_out)), \n",
      "                        dtype = theano.config.floatX)\n",
      "            if activation == T.nnet.sigmoid:\n",
      "                W_value *= 4\n",
      "            W = theano.shared(value = W_value, name = 'W', borrow = True)\n",
      "        if b is None:\n",
      "            b_value = np.zeros((n_out, ), dtype = theano.config.floatX)\n",
      "            b = theano.shared(value = b_value, name = 'b', borrow = True)\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = lin_output if activation is None else activation(lin_output)\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "        \n",
      "def share_data(data, dtype = theano.config.floatX):\n",
      "    shared_data = theano.shared(np.asarray(data, dtype=theano.config.floatX), \n",
      "                                    borrow = True)\n",
      "    return T.cast(shared_data, dtype = dtype)\n",
      "\n",
      "def run_lenet5(v_train_X, v_train_y, v_validation_X, v_validation_y, \n",
      "                learning_rate = 0.01, n_epochs = 200,\n",
      "                batch_size = 500, n_kerns = [20, 50]):\n",
      "    \"\"\"\n",
      "    n_kerns = number of kernels on each layer\n",
      "    \"\"\"\n",
      "    n_train_batches = v_train_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    n_validation_batches = v_validation_X.get_value(borrow = True).shape[0] / batch_size\n",
      "    print '... building the model'\n",
      "    \n",
      "    ## symoblic variables\n",
      "    index = T.lscalar()\n",
      "    x = T.matrix('x')\n",
      "    y = T.ivector('y')\n",
      "    ishape = (28, 28)\n",
      "    rng = np.random.RandomState(0)\n",
      "    \n",
      "    ## reshape matrix of rasterized images of share (batch_size, 28*28)\n",
      "    ## to a 4D tensor, compartible with LeNetConvPoolLayer\n",
      "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
      "    ## construct the first convolutional pooling layer\n",
      "    ## filtering reduces the image size to (28-5+1, 28-5+1) = (24, 24)\n",
      "    ## maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
      "    ## 4D output tensor is thus of shape (batch_size, n_kerns[0], 12, 12)\n",
      "    layer0 = LeNetConvPoolLayer(rng, input = layer0_input, \n",
      "                                image_shape = (batch_size, 1, 28, 28),\n",
      "                                filter_shape = (n_kerns[0], 1, 5, 5),\n",
      "                                poolsize = (2, 2))\n",
      "    ## construct the second convolutional pooling layer\n",
      "    ## filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
      "    ## maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
      "    ## 4D output tensor is thus of shape (n_kerns[0], n_kerns[1], 4, 4)\n",
      "    layer1 = LeNetConvPoolLayer(rng, input=layer0.output,\n",
      "                    image_shape = (batch_size, n_kerns[0], 12, 12),\n",
      "                    filter_shape = (n_kerns[1], n_kerns[0], 5, 5),\n",
      "                    poolsize = (2, 2))\n",
      "    layer2_input = layer1.output.flatten(2)\n",
      "    ## a fully_connected sigmoidal layer\n",
      "    layer2 = HiddenLayer(rng, input=layer2_input, n_in = n_kerns[1]*4*4,\n",
      "                          n_out = 500, activation=T.tanh)\n",
      "    ## classify the values of the fully_connected sigmodial layer\n",
      "    layer3 = LogisticRegression(input = layer2.output, n_in = 500, n_out = 10)\n",
      "    cost = layer3.negative_log_likelihood(y)\n",
      "    \n",
      "    validate_model = theano.function([index], layer3.errors(y),\n",
      "                        givens = {\n",
      "                          x: v_validation_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_validation_y[index*batch_size:(index+1)*batch_size]\n",
      "                      })\n",
      "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
      "    grads = T.grad(cost, params)\n",
      "    updates = [(p, p-learning_rate*gp) for (p, gp) in zip(params, grads)]\n",
      "    train_model = theano.function([index], cost, updates = updates,\n",
      "                    givens = {\n",
      "                          x: v_train_X[index*batch_size:(index+1)*batch_size],\n",
      "                          y: v_train_y[index*batch_size:(index+1)*batch_size]\n",
      "                   })\n",
      "    \n",
      "    print '... training'\n",
      "    patience = 10000\n",
      "    patience_increase = 2\n",
      "    improvement_threshold = 0.995\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "    best_params = None\n",
      "    best_validation_loss = np.inf\n",
      "    start_time = time.clock()\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "            ## if it is time, check the validataion\n",
      "            if (iter+1) % validation_frequency == 0:\n",
      "                validation_loss = np.mean([validate_model(i) for i in xrange(n_validation_batches)])\n",
      "                print 'epoch %i, minibatch %i / %i, validation error %f %%' % (epoch, minibatch_index+1, \n",
      "                        n_train_batches, validation_loss * 100.)\n",
      "                if validation_loss < best_validation_loss:\n",
      "                    if validation_loss < best_validation_loss * improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "                    best_validation_loss = validation_loss\n",
      "                    best_params = params\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "    end_time = time.clock()\n",
      "    print 'Optimization Complete. Best Validation score of %f %%' % (best_validation_loss * 100.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load digits data\n",
      "import cPickle\n",
      "from sklearn.cross_validation import train_test_split\n",
      "X, y = cPickle.load(open('data/digits.pkl', 'rb'))\n",
      "print X.shape, y.shape\n",
      "print np.unique(y)\n",
      "train_X, validation_X, train_y, validation_y = train_test_split(X, y, test_size = 0.2)\n",
      "v_train_X, v_validation_X = share_data(train_X), share_data(validation_X)\n",
      "v_train_y, v_validation_y = share_data(train_y, dtype='int32'), share_data(validation_y, dtype='int32')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(42000, 784) (42000,)\n",
        "[0 1 2 3 4 5 6 7 8 9]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_lenet5(v_train_X, v_train_y, v_validation_X, v_validation_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 67 / 67, validation error 21.625000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 67 / 67, validation error 16.612500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 67 / 67, validation error 13.462500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 67 / 67, validation error 11.537500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 67 / 67, validation error 10.162500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 67 / 67, validation error 9.250000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 67 / 67, validation error 8.512500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 67 / 67, validation error 7.900000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 67 / 67, validation error 7.450000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 67 / 67, validation error 6.862500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 67 / 67, validation error 6.525000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 67 / 67, validation error 6.112500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 67 / 67, validation error 5.937500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 67 / 67, validation error 5.637500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 67 / 67, validation error 5.437500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 67 / 67, validation error 5.325000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 67 / 67, validation error 5.212500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 67 / 67, validation error 5.137500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 67 / 67, validation error 4.875000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 67 / 67, validation error 4.700000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 67 / 67, validation error 4.537500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 67 / 67, validation error 4.312500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 67 / 67, validation error 4.287500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 67 / 67, validation error 4.200000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 67 / 67, validation error 4.150000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 67 / 67, validation error 4.087500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 67 / 67, validation error 3.937500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 67 / 67, validation error 3.887500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 29, minibatch 67 / 67, validation error 3.825000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 30, minibatch 67 / 67, validation error 3.750000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 31, minibatch 67 / 67, validation error 3.725000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 32, minibatch 67 / 67, validation error 3.687500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 33, minibatch 67 / 67, validation error 3.525000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 34, minibatch 67 / 67, validation error 3.512500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 35, minibatch 67 / 67, validation error 3.487500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 36, minibatch 67 / 67, validation error 3.437500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 37, minibatch 67 / 67, validation error 3.362500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 38, minibatch 67 / 67, validation error 3.325000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 39, minibatch 67 / 67, validation error 3.325000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 40, minibatch 67 / 67, validation error 3.200000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 41, minibatch 67 / 67, validation error 3.112500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 42, minibatch 67 / 67, validation error 3.087500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 43, minibatch 67 / 67, validation error 3.187500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 44, minibatch 67 / 67, validation error 3.037500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 45, minibatch 67 / 67, validation error 3.037500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 46, minibatch 67 / 67, validation error 3.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 47, minibatch 67 / 67, validation error 3.087500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 48, minibatch 67 / 67, validation error 2.950000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 49, minibatch 67 / 67, validation error 2.950000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 50, minibatch 67 / 67, validation error 2.925000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 51, minibatch 67 / 67, validation error 2.900000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 52, minibatch 67 / 67, validation error 2.950000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 53, minibatch 67 / 67, validation error 2.887500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 54, minibatch 67 / 67, validation error 2.825000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 55, minibatch 67 / 67, validation error 2.850000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 56, minibatch 67 / 67, validation error 2.812500 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 57, minibatch 67 / 67, validation error 2.787500 %"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-46d6e5ce392b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lenet5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_validation_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-19-b3e17b853362>\u001b[0m in \u001b[0;36mrun_lenet5\u001b[0;34m(v_train_X, v_train_y, v_validation_X, v_validation_y, learning_rate, n_epochs, batch_size, n_kerns)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mminibatch_avg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m## if it is time, check the validataion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/Theano-0.6.0rc3-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}