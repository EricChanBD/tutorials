{
 "metadata": {
  "name": "BASIC_theano"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic Tutorial for Theano deep learning package"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano.tensor as T\n",
      "from theano import function, shared\n",
      "from theano import pp\n",
      "import theano\n",
      "import numpy as np\n",
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Basics of Tensor Functionality***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## create symbols of variables and functions\n",
      "## 1. use T.xx to directly create variables\n",
      "## 2. use operators (such as +) to create new variables - TensorVariable\n",
      "## 3. use function to create function symbols\n",
      "x = T.dscalar('x') ## ALL SYMBOLS must be typed, T.dscalar = 0-d arrays (scalar) of doubles (d)\n",
      "print type(x) ## dscalar is similiar to dtype in np, so x is not an instance of dscalar, but TensorVariable\n",
      "y = T.dscalar('y') ## T.dscalar() is like a factory method\n",
      "print y.type is T.dscalar ## the \"dtype\" of a TensorVariable is accessed by 'type' attr \n",
      "z = x + y\n",
      "print type(z), pp(z) ## pp can be used for pretty-print of variables, but for functions\n",
      "f = function([x, y], z) ## compiling function object to C code, f CAN be used like a normal python function\n",
      "print type(f), f ## cannot use pp for f\n",
      "print f(10.1, 100)\n",
      "print z.eval({x: 10.1, y: 100}) ## z's eval() is equivalent to function(), but less flexible"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.basic.TensorVariable'>\n",
        "True\n",
        "<class 'theano.tensor.basic.TensorVariable'> (x + y)\n",
        "<class 'theano.compile.function_module.Function'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <theano.compile.function_module.Function object at 0x10b918bd0>\n",
        "110.1\n",
        "110.1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## every symbol in theano must be typed, which can be done by using specific factory method in T\n",
      "## matrices algebra\n",
      "x = T.dmatrix('x') # matrix of double- matrices are DUCK-typed, np.array or list of list, it RETURNS np.array\n",
      "print type(x), x.type, pp(x)\n",
      "y = T.dmatrix('y')\n",
      "z = x * y ## elementwise multiplication\n",
      "f = function([x, y], z)\n",
      "## perform f on list of list\n",
      "r = f([[1, 2], [3, 4]], [[10, 20], [30, 40]])\n",
      "print type(r)\n",
      "print r\n",
      "## perform f again on np.array\n",
      "r = f(np.asarray([[1, 2], [3, 4]]), np.asarray([[10, 20], [30, 40]]))\n",
      "print type(r)\n",
      "print r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.basic.TensorVariable'> TensorType(float64, matrix) x\n",
        "<type 'numpy.ndarray'>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[  10.   40.]\n",
        " [  90.  160.]]\n",
        "<type 'numpy.ndarray'>\n",
        "[[  10.   40.]\n",
        " [  90.  160.]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Theano symbol typing system*** \n",
      "\n",
      "- **byte**: bscalar, bvector, bmatrix, brow, bcol, btensor3, btensor4\n",
      "- **16-bit integers**: wscalar, wvector, wmatrix, wrow, wcol, wtensor3, wtensor4\n",
      "- **32-bit integers**: iscalar, ivector, imatrix, irow, icol, itensor3, itensor4\n",
      "- **64-bit integers**: lscalar, lvector, lmatrix, lrow, lcol, ltensor3, ltensor4\n",
      "- **float**: fscalar, fvector, fmatrix, frow, fcol, ftensor3, ftensor4\n",
      "- **double**: dscalar, dvector, dmatrix, drow, dcol, dtensor3, dtensor4\n",
      "- **complex**: cscalar, cvector, cmatrix, crow, ccol, ctensor3, ctensor4\n",
      "\n",
      "**system: scalar, vector, matrix, row, col, tensor3, tensor4**\n",
      "\n",
      "they can be created using the specific type-specific factory methods, or just using the *common* factory methods, \n",
      "such as tensor.scalar(name, dtype=config.floatX)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## shared TensorVariable\n",
      "## Shared TensorVariables are usually used to convert existing python objects to symbol variables\n",
      "## it is more like another type of beast, which is why shared is now in theano pkg directly\n",
      "## instead of in theano.tensor\n",
      "x = shared(np.random.randn(3, 4))\n",
      "print type(x), pp(x), x.type\n",
      "print x.value\n",
      "print T.shape(x)\n",
      "## use eval to get the value in the shared variable\n",
      "print x.eval()\n",
      "## the created variable is a TensorSharedVariable whose .value is a np array in this case\n",
      "## the dtype of x is inferred from the ndarray"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.sharedvar.TensorSharedVariable'> <TensorType(float64, matrix)> TensorType(float64, matrix)\n",
        "(<property object at 0x109aa5e68>,)\n",
        "Shape.0\n",
        "[[ 0.97698495 -1.53368543  0.92842832  0.64082965]\n",
        " [-1.12715379  0.0224872   0.49577723 -0.99701666]\n",
        " [ 0.14669318  1.45799837 -0.38410862  0.23779664]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*The argument to shared variable will NOT be copied, and subsequent changes will be reflected in X.value*\n",
      "\n",
      "*On the other hande, theano makes a copy of any ndarray that you use in an expression, so subsequent changes to that ndarray will not have any effect on the theano expression*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Types of TensorVariables***\n",
      "\n",
      "- *TensorVariable*: The result of symbolic operations typically have this type.\n",
      "- *TensorConstant*: *Python and numpy numbers* are wrapped in this type.\n",
      "- *TensorSharedVariable*: This type is returned by shared() when the value to share is a *numpy ndarray*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***TensorVariable Operation***\n",
      "\n",
      "Tensor pacakage has different types of operators, which can be used to combine different types of variables to create new tensor variables, such as \n",
      "\n",
      "- tensor.reshape, tensor.shape\n",
      "- tensor.flatten\n",
      "- tensor.zeros_like\n",
      "- tensor.stack\n",
      "- tensor.concatenate\n",
      "- tensor.sum\n",
      "- other operators such as indexing\n",
      "- tensor.lt, tensor.gt, and etc\n",
      "- tensor.switch\n",
      "- tensor.dot, tensor.outer\n",
      "- tensor.grad\n",
      "\n",
      "<a href=\"http://deeplearning.net/software/theano/library/tensor/basic.html#libdoc-basic-tensor\">Documentation</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***All about defining functions***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## define function that returns MULTIPLE OUTPUT\n",
      "a, b = T.dmatrices('a', 'b')\n",
      "diff = a - b ## - operator\n",
      "abs_diff = abs(a-b) ## abs operator\n",
      "diff_squared = diff ** 2 ## ** operator\n",
      "f = function([a, b], [diff, abs_diff, diff_squared])\n",
      "pprint(f([[0, 1], [-1, -2]], [[1, 1], [-1, -2]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[-1.,  0.],\n",
        "       [ 0.,  0.]]),\n",
        " array([[ 1.,  0.],\n",
        "       [ 0.,  0.]]),\n",
        " array([[ 1.,  0.],\n",
        "       [ 0.,  0.]])]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## setting a DEFAULT value for an argument\n",
      "## Use theano.Param to WRAP a variable and a default value\n",
      "from theano import Param # Param, Shared are in theano package\n",
      "## Param class allows you to specify properties of your functions\n",
      "## parameters with greater details, such as mutability, default and etc.\n",
      "## Inputs with default values must follow inputs without default values.\n",
      "## Param even allows the overridding of variable name for the specific function.\n",
      "x, y = T.dmatrices('x', 'y')\n",
      "z = x + y\n",
      "f = function([x, Param(y, default=[[1, 2]], name='y_by_name')], z)\n",
      "pprint(f([[2, 3]]))\n",
      "pprint(f([[2, 3]], y_by_name=[[0, 0]])) # use the override name to specify named params\n",
      "pprint(f([[2, 3]], [[0, 0]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "array([[ 3.,  5.]])\n",
        "array([[ 2.,  3.]])\n",
        "array([[ 2.,  3.]])\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Write a function with internal state - using Shared variable\n",
      "## example: accumulator\n",
      "from theano import shared\n",
      "state = shared(0)\n",
      "inc = T.iscalar('inc')\n",
      "accumulator = function([inc], state, updates = [(state, state+inc)]) # inputs, outputs, updates\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(1)\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(10)\n",
      "state.set_value(0) ## reset\n",
      "print 'shared state:', state.get_value()\n",
      "print accumulator(1)\n",
      "\n",
      "## Shared variables are hybrid symbolic and non-symbolic variables whose\n",
      "## value may be shared between multiple functions.\n",
      "## Shared variables can be used in symbolic expressions just like the objects\n",
      "## returned by dmatrices(...) but they also have an internal value that defines\n",
      "## the value taken by this symbolic variable in ALL the functions that use it.\n",
      "\n",
      "## The value stored in the shared variable can be accessed and modified by the \n",
      "## .get_value() and .set_value() methods.\n",
      "\n",
      "## the updates parameter of a Function is a list of pairs of the form \n",
      "## (shared_variable, new_expression) or a dictionary\n",
      "\n",
      "## Main reasons for using shared variable is their efficiency."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shared state: 0\n",
        "0\n",
        "shared state: 1\n",
        "1\n",
        "shared state: 0\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Introducing RANDOMNESS in functions\n",
      "## IN theano a random number generator is implemented as a Random Stream Object\n",
      "## Random Streams are at their core SHARED variables. \n",
      "## Theano random objects are defined and implemented in RandomStreams and at a lower level\n",
      "## RandomStreamBase.\n",
      "## MORE ON RANDOMNESS http://deeplearning.net/software/theano/tutorial/examples.html\n",
      "from theano.tensor.shared_randomstreams import RandomStreams # REMEMBER THIS!\n",
      "## RANDOM STREAM\n",
      "srng = RandomStreams(seed = 0) ## random generator (stream)\n",
      "print type(srng)\n",
      "## RANDOM VARIABLES\n",
      "rv_u = srng.uniform((2, 2)) # random variable, which will update rv_u.rng state everytime\n",
      "rv_n = srng.normal((2, 2))\n",
      "print type(rv_u), rv_u.type, rv_u.rng\n",
      "f = function([], rv_u) ## update srng every time - different values every time\n",
      "g = function([], rv_n, no_default_updates=True) # NOT UPDATING srng - same value every time\n",
      "print f()\n",
      "print f()\n",
      "print g()\n",
      "print g()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'theano.tensor.shared_randomstreams.RandomStreams'>\n",
        "<class 'theano.tensor.basic.TensorVariable'> TensorType(float64, matrix) <RandomStateType>\n",
        "[[ 0.48604732  0.68571232]\n",
        " [ 0.98557605  0.19559641]]\n",
        "[[ 0.58341167  0.98058218]\n",
        " [ 0.1804803   0.70146864]]\n",
        "[[ 1.99759307  0.35128336]\n",
        " [ 1.50384112  1.25808594]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 1.99759307  0.35128336]\n",
        " [ 1.50384112  1.25808594]]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Example of Logistic Regression***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Computing logistic function - elementwise way\n",
      "## definition 1: logistic(x) = 1. / (1. + exp(-x))\n",
      "x = T.dmatrix('x')\n",
      "s = 1 / (1 + T.exp(-x))\n",
      "logistic = function([x], s)\n",
      "print logistic([[0, 1], [-1, -2]])\n",
      "## definition 2: logistic(x) = (1. + tanh(x/2)) / 2\n",
      "s2 = (1 + T.tanh(x/2)) / 2\n",
      "logistic2 = function([x], s2)\n",
      "print logistic2([[0, 1], [-1, -2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.5         0.73105858]\n",
        " [ 0.26894142  0.11920292]]\n",
        "[[ 0.5         0.73105858]\n",
        " [ 0.26894142  0.11920292]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Logistic Regression\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "rng = np.random # random number generator\n",
      "\n",
      "## DATA\n",
      "N = 400\n",
      "feats = 1000\n",
      "data_X, data_y = (rng.randn(N, feats), rng.randint(size = N, low = 0, high = 2))\n",
      "training_steps = 10000\n",
      "print data_X.shape, data_y.shape\n",
      "print np.unique(data_y)\n",
      "\n",
      "## theano symbolic variables\n",
      "x = T.dmatrix('x')\n",
      "y = T.dvector('y') ## y is A VECTOR!!!\n",
      "w = theano.shared(rng.randn(feats), name='w')\n",
      "b = theano.shared(0., name='b')\n",
      "#print 'initial model:'\n",
      "#print w.get_value(), b.get_value()\n",
      "\n",
      "## theano expression graph\n",
      "p_1 = 1 / (1 + T.exp(- T.dot(x, w) - b))\n",
      "prediction = p_1 > 0.5\n",
      "x_ent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)\n",
      "cost = T.mean(x_ent) + 0.01 * T.sum(abs(w)) # cost = xent.mean() + 0.01 * (w ** 2).sum()\n",
      "accuracy = T.mean(T.eq(prediction, y))\n",
      "gw, gb = T.grad(cost, [w, b])\n",
      "\n",
      "## compile - shared w, b for train and predict function\n",
      "train = theano.function(\n",
      "            inputs = [x, y],\n",
      "            outputs = [prediction, x_ent],\n",
      "            updates = {w: w - 0.1*gw, b: b-0.1*gb})\n",
      "predict = theano.function(inputs = [x], outputs = prediction)\n",
      "score = theano.function(inputs = [x, y], outputs = accuracy)\n",
      "\n",
      "## train\n",
      "for i in xrange(training_steps):\n",
      "    if i % 1000 == 0:\n",
      "        print 'iteration ', i \n",
      "    pred, err = train(data_X, data_y)\n",
      "    \n",
      "## predict\n",
      "print 'performance on D:', score(data_X, data_y)\n",
      "print 'significant number of features:', sum(np.abs(w.get_value()) > 0) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(400, 1000) (400,)\n",
        "[0 1]\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "performance on D:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0\n",
        "significant number of features: 1000\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Wrap theano logistic regression as sklearn model\n",
      "from sklearn.base import BaseEstimator\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "class LogisticRegression(BaseEstimator):\n",
      "    def __init__(self, alpha = 0.01, n_iters = 10000, learning_rate = 0.1):\n",
      "        ## meta params\n",
      "        self.rng = np.random\n",
      "        self.alpha = alpha\n",
      "        self.n_iters = n_iters\n",
      "        self.learning_rate = 0.1\n",
      "        ## independent symbols\n",
      "        X = T.dmatrix('X')\n",
      "        y = T.dvector('y')\n",
      "        self.w = theano.shared(rng.randn(1), 'w') # we dont know the dim yet\n",
      "        self.b = theano.shared(0., 'b')\n",
      "        ## dependent expressions\n",
      "        p_1 = 1 / (1 + T.exp(- T.dot(X, self.w) - self.b))\n",
      "        prediction = p_1 > 0.5\n",
      "        x_ent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)\n",
      "        cost = T.mean(x_ent) + self.alpha * T.sum(self.w**2)\n",
      "        accuracy = T.mean(T.eq(y, prediction))\n",
      "        gw, gb = T.grad(cost, [self.w, self.b])\n",
      "        ## functions\n",
      "        self.train = theano.function(\n",
      "                        inputs = [X, y],\n",
      "                        outputs = [prediction, x_ent],\n",
      "                        updates = ((self.w, self.w - self.learning_rate * gw),\n",
      "                                   (self.b, self.b - self.learning_rate * gb)))\n",
      "        self.predict = theano.function(\n",
      "                        inputs = [X],\n",
      "                        outputs = prediction)\n",
      "        self.predict_prob = theano.function(\n",
      "                        inputs = [X],\n",
      "                        outputs = p_1)\n",
      "        self.score = theano.function(\n",
      "                        inputs = [X, y],\n",
      "                        outputs = accuracy)\n",
      "    def fit(self, X, y):\n",
      "        ## intialize w, and b values\n",
      "        n_samples, n_feats = X.shape\n",
      "        self.w.set_value(self.rng.randn(n_feats))\n",
      "        ## train on self.w and self.b\n",
      "        for i in xrange(self.n_iters):\n",
      "            if i % 1000 == 0:\n",
      "                print 'iteration:', i\n",
      "            self.train(X, y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        return self.predict(X)\n",
      "    def predict_prob(self, X):\n",
      "        return self.predict_prob(X)\n",
      "    def score(self, X, y):\n",
      "        return self.score(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "from sklearn.cross_validation import train_test_split\n",
      "data_X, data_y = cPickle.load(open('data/blackbox.pkl', 'rb'))\n",
      "data_y[data_y != 1] = 0 # binary classification\n",
      "train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size = 0.2)\n",
      "\n",
      "\n",
      "lr = LogisticRegression()\n",
      "lr.fit(train_X, train_y)\n",
      "print lr.score(test_X, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "0.77"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## parallel run\n",
      "from IPython.parallel import Client\n",
      "client = Client()\n",
      "dv = client[:]\n",
      "print 'runing on ', len(dv)\n",
      "dv.block = True\n",
      "dv['train_X'] = train_X\n",
      "dv['train_y'] = train_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "runing on  4\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%load 80\n",
      "lr = LogisticRegression()\n",
      "lr.fit(train_X, train_y)\n",
      "print lr.score(train_X, train_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9000\n",
        "0.81625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Flow of Control***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## CONDITIONING\n",
      "## two main ops: theano.ifelse.ifelse or T.switch\n",
      "## ifelse => takes a boolean condition and two variables, lazy eval of one variable (returned)\n",
      "## switch => takes a tensor as condition and two variables. It is eleementwise and more general\n",
      "\n",
      "## NOTE: Unless linker='vm' or linker='cvm' are used, \n",
      "## ifelse will compute both variables and take the same computation time as switch. \n",
      "## Although the linker is not currently set by default to cvm, it will be in the near future\n",
      "from theano.ifelse import ifelse\n",
      "import time\n",
      "a, b = T.scalars('a', 'b')\n",
      "x, y = T.matrices('x', 'y')\n",
      "\n",
      "z_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y)) # evaluate both means of x and y\n",
      "z_lazy = ifelse(T.lt(a, b), T.mean(x), T.mean(y)) # evaluate one of them\n",
      "\n",
      "f_switch = theano.function([a, b, x, y], z_switch, mode = theano.Mode(linker = 'vm'))\n",
      "f_lazyifelse = theano.function([a, b, x, y], z_lazy, mode = theano.Mode(linker = 'vm'))\n",
      "\n",
      "val1, val2 = 0., 1.\n",
      "big_mat1, big_mat2 = np.ones((10000, 1000)), np.ones((10000, 1000))\n",
      "\n",
      "%timeit f_switch(val1, val2, big_mat1, big_mat2)\n",
      "%timeit f_lazyifelse(val1, val2, big_mat1, big_mat2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 22.2 ms per loop\n",
        "100 loops, best of 3: 10.9 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## LOOPS\n",
      "## The most general way of doing loop in theano is through the scan op\n",
      "## Both reduction and map can be viewed as special cases of scan\n",
      "## How it works:\n",
      "## The op scans a function along some input sequence, producing an output at each time-step\n",
      "## the function can see the previous K time steps of the function\n",
      "## Unchanging variables are passed to scan as non_sequences. Initialization occurs\n",
      "## in outputs_info. And the accumulation happens automatically\n",
      "## The general order of function parameter to fn param in scan is:\n",
      "## sequences (if any), prior result(s) (if needed), non-sequences (if any)\n",
      "\n",
      "## ELEMENTWISE POWER\n",
      "k = T.iscalar('k')\n",
      "A = T.vector('A') ## float\n",
      "def inner_fct(prior_result, B):\n",
      "    return prior_result * B\n",
      "## Symbolic description of the result - specially define the UPDATES steps\n",
      "result, updates = theano.scan(fn = inner_fct, outputs_info=T.ones_like(A), \n",
      "                            non_sequences=A, n_steps = k)\n",
      "## Scan has provided us with A ** 1 through A ** k.  Keep only the last\n",
      "## value. Scan notices this and does not waste memory saving them.\n",
      "final_result = result[-1]\n",
      "power = theano.function(inputs = [A, k], outputs = final_result, updates = updates)\n",
      "print power(range(10), 2)\n",
      "\n",
      "## POLYNOMIAL\n",
      "coefficents = T.vector('coefficients')\n",
      "x = T.scalar('x')\n",
      "## symbolic representation of polynomial components and updates\n",
      "## Be careful with the initial prior T.constant(0.0), dtype need to be \n",
      "## provided as coefficents.dtype otherwise a downcasting error will happen\n",
      "results, updates = theano.scan(fn = lambda coef, power, prior, x: (power+1, prior+coef*(x**power)),\n",
      "                                    outputs_info = [0., T.as_tensor_variable(np.asarray(0., coefficents.dtype))],\n",
      "                                                        #T.constant(0.0, dtype=coefficents.dtype)],\n",
      "                                    sequences = coefficents,\n",
      "                                    non_sequences = x)\n",
      "result = results[1][-1] # results = [seq(power), seq(sum_of_components)]\n",
      "poly = theano.function(inputs = [coefficents, x], outputs = result, updates = updates)\n",
      "print poly([1., 2, 3], 2)\n",
      "print sum([2 ** i * c for (i, c) in enumerate([1, 2, 3])])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
        "17.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17\n"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## YET ANOTHER POLYNOMIAL\n",
      "power_coeff_pairs = T.matrix('power_coeff_pairs')\n",
      "x = T.scalar('x')\n",
      "results, updates = theano.scan(fn = lambda power_coeff, x: power_coeff[1] * (x ** power_coeff[0]),\n",
      "                                outputs_info = None,\n",
      "                                sequences = power_coeff_pairs,\n",
      "                                non_sequences = x)\n",
      "result = T.sum(results)\n",
      "poly = theano.function(inputs = [power_coeff_pairs, x], outputs = result, updates = updates)\n",
      "print poly(list(enumerate([1, 2, 3])), 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17.0\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T.arange(1, 10).eval()\n",
      "T.as_tensor_variable?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Sparse Matrices in Theano***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Theano sparse matrices are based on scipy sparse package, \n",
      "## it currently supports two types, namely csc and csr formats, for fast linear algebra\n",
      "## A general rule of choosing between csr and csc formats is:\n",
      "## If shape[0] > shape[1], use csr format. Otherwise, use csc.\n",
      "## ANOTHER ONE is: Use the format compatible with the ops in your computation graph.\n",
      "import scipy.sparse as sp\n",
      "from theano import sparse\n",
      "print sparse.all_dtypes\n",
      "\n",
      "## MOVE FROM and TO dense matrices\n",
      "x = sparse.csc_matrix('x')\n",
      "print x.type\n",
      "print sparse.dense_from_sparse(x).type\n",
      "print sparse.csr_from_dense(sparse.dense_from_sparse(x)).type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['uint64', 'int32', 'int16', 'complex128', 'complex64', 'float64', 'uint8', 'uint32', 'uint16', 'int64', 'int8', 'float32'])\n",
        "Sparse[float64, csc]\n",
        "TensorType(float64, matrix)\n",
        "Sparse[float64, csr]\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label,pixel0,pixel1,pixel2,pixel3,pixel4,pixel5,pixel6,pixel7,pixel8,pixel9,pixel10,pixel11,pixel12,pixel13,pixel14,pixel15,pixel16,pixel17,pixel18,pixel19,pixel20,pixel21,pixel22,pixel23,pixel24,pixel25,pixel26,pixel27,pixel28,pixel29,pixel30,pixel31,pixel32,pixel33,pixel34,pixel35,pixel36,pixel37,pixel38,pixel39,pixel40,pixel41,pixel42,pixel43,pixel44,pixel45,pixel46,pixel47,pixel48,pixel49,pixel50,pixel51,pixel52,pixel53,pixel54,pixel55,pixel56,pixel57,pixel58,pixel59,pixel60,pixel61,pixel62,pixel63,pixel64,pixel65,pixel66,pixel67,pixel68,pixel69,pixel70,pixel71,pixel72,pixel73,pixel74,pixel75,pixel76,pixel77,pixel78,pixel79,pixel80,pixel81,pixel82,pixel83,pixel84,pixel85,pixel86,pixel87,pixel88,pixel89,pixel90,pixel91,pixel92,pixel93,pixel94,pixel95,pixel96,pixel97,pixel98,pixel99,pixel100,pixel101,pixel102,pixel103,pixel104,pixel105,pixel106,pixel107,pixel108,pixel109,pixel110,pixel111,pixel112,pixel113,pixel114,pixel115,pixel116,pixel117,pixel118,pixel119,pixel120,pixel121,pixel122,pixel123,pixel124,pixel125,pixel126,pixel127,pixel128,pixel129,pixel130,pixel131,pixel132,pixel133,pixel134,pixel135,pixel136,pixel137,pixel138,pixel139,pixel140,pixel141,pixel142,pixel143,pixel144,pixel145,pixel146,pixel147,pixel148,pixel149,pixel150,pixel151,pixel152,pixel153,pixel154,pixel155,pixel156,pixel157,pixel158,pixel159,pixel160,pixel161,pixel162,pixel163,pixel164,pixel165,pixel166,pixel167,pixel168,pixel169,pixel170,pixel171,pixel172,pixel173,pixel174,pixel175,pixel176,pixel177,pixel178,pixel179,pixel180,pixel181,pixel182,pixel183,pixel184,pixel185,pixel186,pixel187,pixel188,pixel189,pixel190,pixel191,pixel192,pixel193,pixel194,pixel195,pixel196,pixel197,pixel198,pixel199,pixel200,pixel201,pixel202,pixel203,pixel204,pixel205,pixel206,pixel207,pixel208,pixel209,pixel210,pixel211,pixel212,pixel213,pixel214,pixel215,pixel216,pixel217,pixel218,pixel219,pixel220,pixel221,pixel222,pixel223,pixel224,pixel225,pixel226,pixel227,pixel228,pixel229,pixel230,pixel231,pixel232,pixel233,pixel234,pixel235,pixel236,pixel237,pixel238,pixel239,pixel240,pixel241,pixel242,pixel243,pixel244,pixel245,pixel246,pixel247,pixel248,pixel249,pixel250,pixel251,pixel252,pixel253,pixel254,pixel255,pixel256,pixel257,pixel258,pixel259,pixel260,pixel261,pixel262,pixel263,pixel264,pixel265,pixel266,pixel267,pixel268,pixel269,pixel270,pixel271,pixel272,pixel273,pixel274,pixel275,pixel276,pixel277,pixel278,pixel279,pixel280,pixel281,pixel282,pixel283,pixel284,pixel285,pixel286,pixel287,pixel288,pixel289,pixel290,pixel291,pixel292,pixel293,pixel294,pixel295,pixel296,pixel297,pixel298,pixel299,pixel300,pixel301,pixel302,pixel303,pixel304,pixel305,pixel306,pixel307,pixel308,pixel309,pixel310,pixel311,pixel312,pixel313,pixel314,pixel315,pixel316,pixel317,pixel318,pixel319,pixel320,pixel321,pixel322,pixel323,pixel324,pixel325,pixel326,pixel327,pixel328,pixel329,pixel330,pixel331,pixel332,pixel333,pixel334,pixel335,pixel336,pixel337,pixel338,pixel339,pixel340,pixel341,pixel342,pixel343,pixel344,pixel345,pixel346,pixel347,pixel348,pixel349,pixel350,pixel351,pixel352,pixel353,pixel354,pixel355,pixel356,pixel357,pixel358,pixel359,pixel360,pixel361,pixel362,pixel363,pixel364,pixel365,pixel366,pixel367,pixel368,pixel369,pixel370,pixel371,pixel372,pixel373,pixel374,pixel375,pixel376,pixel377,pixel378,pixel379,pixel380,pixel381,pixel382,pixel383,pixel384,pixel385,pixel386,pixel387,pixel388,pixel389,pixel390,pixel391,pixel392,pixel393,pixel394,pixel395,pixel396,pixel397,pixel398,pixel399,pixel400,pixel401,pixel402,pixel403,pixel404,pixel405,pixel406,pixel407,pixel408,pixel409,pixel410,pixel411,pixel412,pixel413,pixel414,pixel415,pixel416,pixel417,pixel418,pixel419,pixel420,pixel421,pixel422,pixel423,pixel424,pixel425,pixel426,pixel427,pixel428,pixel429,pixel430,pixel431,pixel432,pixel433,pixel434,pixel435,pixel436,pixel437,pixel438,pixel439,pixel440,pixel441,pixel442,pixel443,pixel444,pixel445,pixel446,pixel447,pixel448,pixel449,pixel450,pixel451,pixel452,pixel453,pixel454,pixel455,pixel456,pixel457,pixel458,pixel459,pixel460,pixel461,pixel462,pixel463,pixel464,pixel465,pixel466,pixel467,pixel468,pixel469,pixel470,pixel471,pixel472,pixel473,pixel474,pixel475,pixel476,pixel477,pixel478,pixel479,pixel480,pixel481,pixel482,pixel483,pixel484,pixel485,pixel486,pixel487,pixel488,pixel489,pixel490,pixel491,pixel492,pixel493,pixel494,pixel495,pixel496,pixel497,pixel498,pixel499,pixel500,pixel501,pixel502,pixel503,pixel504,pixel505,pixel506,pixel507,pixel508,pixel509,pixel510,pixel511,pixel512,pixel513,pixel514,pixel515,pixel516,pixel517,pixel518,pixel519,pixel520,pixel521,pixel522,pixel523,pixel524,pixel525,pixel526,pixel527,pixel528,pixel529,pixel530,pixel531,pixel532,pixel533,pixel534,pixel535,pixel536,pixel537,pixel538,pixel539,pixel540,pixel541,pixel542,pixel543,pixel544,pixel545,pixel546,pixel547,pixel548,pixel549,pixel550,pixel551,pixel552,pixel553,pixel554,pixel555,pixel556,pixel557,pixel558,pixel559,pixel560,pixel561,pixel562,pixel563,pixel564,pixel565,pixel566,pixel567,pixel568,pixel569,pixel570,pixel571,pixel572,pixel573,pixel574,pixel575,pixel576,pixel577,pixel578,pixel579,pixel580,pixel581,pixel582,pixel583,pixel584,pixel585,pixel586,pixel587,pixel588,pixel589,pixel590,pixel591,pixel592,pixel593,pixel594,pixel595,pixel596,pixel597,pixel598,pixel599,pixel600,pixel601,pixel602,pixel603,pixel604,pixel605,pixel606,pixel607,pixel608,pixel609,pixel610,pixel611,pixel612,pixel613,pixel614,pixel615,pixel616,pixel617,pixel618,pixel619,pixel620,pixel621,pixel622,pixel623,pixel624,pixel625,pixel626,pixel627,pixel628,pixel629,pixel630,pixel631,pixel632,pixel633,pixel634,pixel635,pixel636,pixel637,pixel638,pixel639,pixel640,pixel641,pixel642,pixel643,pixel644,pixel645,pixel646,pixel647,pixel648,pixel649,pixel650,pixel651,pixel652,pixel653,pixel654,pixel655,pixel656,pixel657,pixel658,pixel659,pixel660,pixel661,pixel662,pixel663,pixel664,pixel665,pixel666,pixel667,pixel668,pixel669,pixel670,pixel671,pixel672,pixel673,pixel674,pixel675,pixel676,pixel677,pixel678,pixel679,pixel680,pixel681,pixel682,pixel683,pixel684,pixel685,pixel686,pixel687,pixel688,pixel689,pixel690,pixel691,pixel692,pixel693,pixel694,pixel695,pixel696,pixel697,pixel698,pixel699,pixel700,pixel701,pixel702,pixel703,pixel704,pixel705,pixel706,pixel707,pixel708,pixel709,pixel710,pixel711,pixel712,pixel713,pixel714,pixel715,pixel716,pixel717,pixel718,pixel719,pixel720,pixel721,pixel722,pixel723,pixel724,pixel725,pixel726,pixel727,pixel728,pixel729,pixel730,pixel731,pixel732,pixel733,pixel734,pixel735,pixel736,pixel737,pixel738,pixel739,pixel740,pixel741,pixel742,pixel743,pixel744,pixel745,pixel746,pixel747,pixel748,pixel749,pixel750,pixel751,pixel752,pixel753,pixel754,pixel755,pixel756,pixel757,pixel758,pixel759,pixel760,pixel761,pixel762,pixel763,pixel764,pixel765,pixel766,pixel767,pixel768,pixel769,pixel770,pixel771,pixel772,pixel773,pixel774,pixel775,pixel776,pixel777,pixel778,pixel779,pixel780,pixel781,pixel782,pixel783\r",
        "\r\n",
        "1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,188,255,94,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,191,250,253,93,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,123,248,253,167,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,247,253,208,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,29,207,253,235,77,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,54,209,253,253,88,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,93,254,253,238,170,17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,210,254,253,159,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,209,253,254,240,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,27,253,253,254,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,206,254,254,198,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,168,253,253,196,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,203,253,248,76,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,22,188,253,245,93,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,103,253,253,191,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89,240,253,195,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,220,253,253,80,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,94,253,253,253,94,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89,251,253,250,131,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,214,218,95,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}